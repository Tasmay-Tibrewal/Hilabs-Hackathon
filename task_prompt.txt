i am attaching the whole info and task description, we need to present our solution for the hackathon tommorrow, can you create a very good looking website which we can deploy, with all the details and explanation, with diagrams and tables, etc. note: maintain conciseness of flow and things like going to next topic directly, but also keep an option to view in more detailed, say keep a question mark (?) which when hovered over opens a window with more specific info, or some button which opens a new section/pop-up with something more, or say a short code snippet. so keep the website both small, easy to navigate, detailed enough in first look and highly expandable for going in depth.
add things like future works and all as well. and keep the webpage design professional and good, keep color pallete simple black and white, like openai/uber. but use nice design and fun elements.
i am attaching all information here ask me any question if needed.

problem statement:
HiLabs Hackathon 2025: Harmonizing Clinical Concepts
Introduction
HiLabs invites IIT Kharagpur students to participate in an exciting hackathon cantered on
solving one of the biggest challenges in healthcare data science: clinical data harmonization.
Every day, hospitals, labs, and pharmacies generate millions of clinical records. These
records often come coded in different systems, with inconsistent descriptions, or even with
errors. For example:
• A blood test for Haemoglobin might be sent as “Hb” by one lab and “HGB” by another.
• A medication like Paracetamol might be recorded as “Acetaminophen” or with a local
drug code.
• A clinical condition such as Asthma could be described in multiple inconsistent forms
across systems.
This variation makes it difficult to standardize healthcare data and use it for analytics,
decision-making, and regulatory reporting. The solution is to map all such inputs to standard
clinical terminologies — RxNorm (for medications), and SNOMED CT (for diagnoses,
procedures and labs).
The Challenge
Your task is to build an intelligent clinical data harmonization engine that:
1. Takes raw clinical inputs (clinical entities).
2. Maps them to standardized codes in RxNorm, or SNOMED CT, based on the entity
types.
3. Outputs clean, harmonized results that can be consumed downstream.
The inputs you receive may not always be clean: some descriptions may be incomplete, and
some records may only have partial information. Your solution must be able to handle these
variations and still map the inputs to the correct standardized concept wherever possible.
What We Provide
• Sample clinical dataset with descriptions from labs, medications, diagnoses &
procedures in mixed/unclean formats.
• Target code and description mappings of RxNorm, and SNOMED CT.
• Test cases with a list of raw clinical inputs (with their entities)
Rules
• Only open-source libraries and frameworks are allowed.
• No external APIs or proprietary datasets can be used.
• Submissions must include:
o Output codes, descriptions, coding system name
o A code repository (public).
o A run script and README with instructions.
o A short demo or notebook walkthrough of your solution.
Evaluation Criteria
• Accuracy of Mappings (50%): Correctness in mapping input descriptions to correct
codes and descriptions in RxNorm or SNOMED.
• Technical Approach (25%): Quality of algorithms, efficiency of handling variations, and
robustness.
• Innovation & Creativity (15%): Novel ideas for handling ambiguity or messy inputs.
• Completeness & Documentation (10%): Clear setup, runnable code, and end-to-end
solution.
Sample Input-Output Mapping
Input Output Remarks
Input
Description
Entity
Type
Standard
Code
Standard
System
Standard
Description
Paracetamol
500 mg
Medication 198440 RXNORM Acetaminophen
500 MG Oral
Tablet
Paracetamol is
general name of
Acetaminophen
Aspirin 10
mg
Medication 1191 RXNORM Aspirin Aspirin 10 mg does
not exist as a
standard form; falls
back on base
medication aspirin
Asthma Diagnosis 195967001 SNOMED
CT
Asthma Direct match
persistent
pain on the
upper right
portion of
your
abdomen
Diagnosis 285388000 SNOMED
CT
Right sided
abdominal pain
natural language
input correctly
parsed to identify
the correct location
of pain
Chest xr Procedure 399208008 SNOMED
CT
Plain X-ray of
chest
Abbreviation "xr"
expanded to "X-ray"
for proper matching
Appendix
removal
surgery
Procedure 80146002 SNOMED
CT
Appendectomy Common layman
term mapped to its
standard medical
procedure name.
Bilirubin Lab 359986008 SNOMED
CT
Bilirubin, total
measurement
Lab test name
normalized to its
standard
measurement
terminology,
instead of being
mapped to
substance
Bilirubin with
SNOMED code of
79706000.
fasting
sugar test
Lab 52302001 SNOMED
CT
Glucose
measurement,
fasting
Common term
mapped to clinical
terminology

original ps's md:
# Harmonizing Clinical Concepts

## Repository Overview
This repository contains all the necessary files and instructions required for this problem statement on harmonizing clinical concepts.

## Contents of the Zip Folder
- **`Test.xlsx`**  
  Excel file containing input data. Your predictions will be appended to this file.

- **`snomed_all_data.parquet`**  
  Parquet file containing all target codes and descriptions for the SNOMED CT coding system.

- **`rxnorm_all_data.parquet`**  
  Parquet file containing all target codes and descriptions for the RxNorm coding system.

- **`Column Reference Guide.md`**  
  Markdown file explaining the columns present in the SNOMED CT and RxNorm parquet files.

## Submission Guidelines
To complete your submission:

1. **Modify the `Test.xlsx` file** by appending the following three columns:
   - `Output Coding System`: Either `SNOMEDCT_US` or `RXNORM`
   - `Output Target Code`: Predicted code in **string** format
   - `Output Target Description`: Human-readable description of the predicted code

2. **Your solution in a public code repository.**

3. **Ensure your repository includes:**
   - A clear, concise `README.md` explaining:
     - Your technical solution
     - A short walkthrough of the repository
   - Code that can be easily tested with additional input files

and

# Columns Reference Guide

---

## 1. `CUI` — Concept Unique Identifier
- **Definition**: A unique identifier assigned to a concept.  
- **Purpose**: Groups together all terms from different vocabularies (SNOMED CT, RxNorm, ICD, etc.) that mean the same thing (are synonymous).  
- **Format**: Always starts with a `"C"` followed by 7 digits (e.g., `C0011849`).  
- **Example**:
  - `C0011849` → *Diabetes Mellitus*  
    - In RxNorm: `Diabetes Mellitus`  
    - In SNOMED CT: `44054006 | Diabetes mellitus (disorder) |`

---

## 2. `System` — Source Vocabulary (SAB)
- **Definition**: Abbreviation for the source vocabulary or terminology that contributed the concept.  
- **Purpose**: Identifies where the term came from (RxNorm, SNOMED CT, MeSH, ICD-10, etc.).  
- **Example**:
  - `RXNORM` → concepts from the RxNorm vocabulary.  
  - `SNOMEDCT_US` → concepts from the U.S. edition of SNOMED CT.  

---

## 3. `TTY` — Term Type
- **Definition**: Describes the role of the term within the source vocabulary.  
- **Purpose**: Differentiates between preferred names, synonyms, codes, ingredients, brand names, etc.  
- **Common Values**:
  - `PT` (SNOMED CT) → Preferred Term.  
  - `SY` (RxNorm, SNOMED CT) → Synonym.  
  - `SCD` (RxNorm) → Semantic Clinical Drug (normalized clinical drug concept).  
  - `BN` (RxNorm) → Brand Name.  
- **Example**:
  - RxNorm:  
    - `SCD` → *Metformin 500 MG Oral Tablet*  
    - `BN` → *Glucophage*  
  - SNOMED CT:  
    - `PT` → *Diabetes mellitus (disorder)*  
    - `SY` → *Sugar diabetes*  

---

## 4. `CODE` — Source Code
- **Definition**: The identifier used **within the source vocabulary** for the concept.  
- **Purpose**: Provides the original vocabulary-specific code.  
- **Format**: Varies by vocabulary.  
- **Example**:
  - RxNorm: `860975` → *Metformin 500 MG Oral Tablet*  
  - SNOMED CT: `44054006` → *Diabetes mellitus (disorder)*  

---

## 5. `STR` — String (Term Name)
- **Definition**: The actual human-readable name of the concept as it appears in the source vocabulary.  
- **Purpose**: Stores the textual representation of the concept.  
- **Example**:
  - RxNorm: `Metformin 500 MG Oral Tablet`  
  - SNOMED CT: `Diabetes mellitus (disorder)`  

---

## 6. `STY` — Semantic Type
- **Definition**: The broad category or semantic group.  
- **Purpose**: Provides higher-level grouping of concepts beyond source vocabularies.  
- **Examples**:
  - `T047` → *Disease or Syndrome*  
  - `T121` → *Pharmacologic Substance*  
- **Example**:
  - `C0011849` (*Diabetes Mellitus*) → `Disease or Syndrome`  
  - `C0025598` (*Metformin*) → `Pharmacologic Substance`  

---

# Summary Table

| Column  | Meaning                                | Example (RxNorm)                   | Example (SNOMED CT)                    |
|---------|----------------------------------------|------------------------------------|----------------------------------------|
| `CUI`   | Concept Unique Identifier              | `C0025598` (*Metformin*)           | `C0011849` (*Diabetes Mellitus*)       |
| `System`| Source vocabulary abbreviation (SAB)   | `RXNORM`                           | `SNOMEDCT_US`                          |
| `TTY`   | Term type in source vocabulary         | `SCD` (Semantic Clinical Drug)     | `PT` (Preferred Term)                  |
| `CODE`  | Code in source vocabulary              | `860975`                           | `44054006`                             |
| `STR`   | Human-readable string/term             | *Metformin 500 MG Oral Tablet*     | *Diabetes mellitus (disorder)*         |
| `STY`   | Semantic type                          | *Pharmacologic Substance*          | *Disease or Syndrome*                  |

---


code:
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
HiLabs 2025 — Clinical Concept Harmonizer (Semantic + Fuzzy + LLM-Assist)

Adds:
- LLM-based query expansion (alternate keywords, description, likely STYs) via GPT-OSS-20B
- Weighted multi-signal scoring (desc/keywords/direct/STY)
- Large-K pipeline with fuzzy re-rank and per-code aggregation score
- Optional LLM re-ranking of final top codes with reasoning (XML)
- Keeps previous non-LLM pipeline switchable
"""

import argparse
import json
import os
os.environ.setdefault("PYTORCH_CUDA_ALLOC_CONF", "expandable_segments:True")

from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass

import re
import gc
import math
import numpy as np
import pandas as pd
from tqdm import tqdm

import faiss
import torch
from sentence_transformers import SentenceTransformer

try:
    from rapidfuzz import fuzz
    HAVE_RAPIDFUZZ = True
except Exception:
    HAVE_RAPIDFUZZ = False

from huggingface_hub import login as hf_login, hf_hub_download

import asyncio
import concurrent.futures
import multiprocessing
import uuid
import inspect
from datetime import datetime

try:
    from threadpoolctl import threadpool_limits
except Exception:
    threadpool_limits = None

from contextlib import nullcontext

DATE_FORMAT_STR = "%Y-%m-%d %H:%M:%S"

def gpu_summary():
    if not torch.cuda.is_available():
        return 0, 0, 0
    gpus = torch.cuda.device_count()
    total = 0
    max_per = 0
    for i in range(gpus):
        props = torch.cuda.get_device_properties(i)
        mem = getattr(props, "total_memory", 0)
        total += mem
        max_per = max(max_per, mem)
    return gpus, total, max_per  # counts, sum bytes, max bytes


def default_concurrency():
    g, _, _ = gpu_summary()
    if g > 0:
        return 32 * g
    # CPU fallback
    cores = max(1, multiprocessing.cpu_count())
    return max(1, 2 * (cores - 1))

# -------------------- Constants & files --------------------
SNOMED = "SNOMEDCT_US"
RXNORM  = "RXNORM"

META_NAME          = "meta.json"
SNOMED_INDEX_NAME  = "snomed.index.faiss"
RXNORM_INDEX_NAME  = "rxnorm.index.faiss"
SNOMED_CAT_NAME    = "snomed_catalog.parquet"
RXNORM_CAT_NAME    = "rxnorm_catalog.parquet"
STY_VOCAB_JSON     = "sty_vocab.json"
STY_EMB_NPY        = "sty_embeddings.npy"

REQ_INPUT_COL = "Input Entity Description"
REQ_TYPE_COL  = "Entity Type"

OUT_SYS_COL   = "Output Coding System"
OUT_CODE_COL  = "Output Target Code"
OUT_DESC_COL  = "Output Target Description"
OUT_STY_COL   = "Output Semantic Type (STY)"
OUT_TTY_COL   = "Output Term Type (TTY)"

# ---------- Default Unsloth model choices ----------
# GGUF for llama.cpp (CPU or forced)
UNSLOTH_GGUF_QWEN_REPO = "unsloth/Qwen3-4B-Instruct-2507-GGUF"
UNSLOTH_GGUF_QWEN_FILE = "Qwen3-4B-Instruct-2507-UD-Q4_K_XL.gguf"

# vLLM (HF-style) repos – adjust if your org uses different names.
UNSLOTH_VLLM_OSS_20B = "Qwen/Qwen3-4B-Instruct-2507"  # GPT-OSS-20B HF weights
UNSLOTH_VLLM_QWEN4B  = "Qwen/Qwen3-4B-Instruct-2507"  # Qwen3 4B HF weights

# Default vLLM quantization to "bitsandbytes" (bnb)
DEFAULT_VLLM_QUANT = "bitsandbytes"
DEFAULT_VLLM_QUANT_QWEN4B = "bitsandbytes"

# -------------------- HF login --------------------
def maybe_hf_login(token: Optional[str]):
    if token:
        try:
            hf_login(token=token)
            print("[HF] Logged in to HuggingFace Hub.")
        except Exception as e:
            print(f"[HF] Login failed: {e}")

def pick_llm_backend_and_model(backend, hf_model_id, gguf_path):
    # Respect explicit backend choice first
    if backend in ("vllm", "llama-cpp"):
        if backend == "vllm":
            if gguf_path:
                print(f"[LLM {datetime.now().strftime(DATE_FORMAT_STR)}] Note: gguf_path is ignored by vLLM (GGUF is llama.cpp-only).")
            # default Unsloth vLLM target if none supplied
            if not hf_model_id or hf_model_id in ("auto", ""):
                hf_model_id = UNSLOTH_VLLM_OSS_20B
        else:  # llama-cpp
            if not gguf_path or gguf_path in ("auto", ""):
                gguf_path = hf_hub_download(
                    repo_id=UNSLOTH_GGUF_QWEN_REPO, filename=UNSLOTH_GGUF_QWEN_FILE
                )
        return backend, hf_model_id, gguf_path

    # backend == auto
    g, total_bytes, max_bytes = gpu_summary()
    max_gb = max_bytes / (1024**3)

    if g == 0:
        # CPU → llama.cpp + Qwen3 4B GGUF
        if not gguf_path:
            gguf_path = hf_hub_download(
                repo_id=UNSLOTH_GGUF_QWEN_REPO, filename=UNSLOTH_GGUF_QWEN_FILE
            )
        return "llama-cpp", None, gguf_path

    # GPU present → vLLM
    # If max per-GPU VRAM < 22 GB → prefer Qwen3 4B HF (bnb)
    if max_gb < 22.0:
        return "vllm", (hf_model_id or UNSLOTH_VLLM_QWEN4B), None

    # Otherwise prefer GPT-OSS-20B HF (bnb)
    return "vllm", (hf_model_id or UNSLOTH_VLLM_OSS_20B), None

# -------------------- CUDA memory helpers --------------------
def clear_cuda_memory(note: str = ""):
    try:
        if torch.cuda.is_available():
            torch.cuda.synchronize()
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()
    except Exception:
        pass
    gc.collect()
    if note:
        try:
            if torch.cuda.is_available():
                alloc = torch.cuda.memory_allocated() / (1024**3)
                reserv = torch.cuda.memory_reserved() / (1024**3)
                print(f"[CUDA] Cleared {note} | allocated={alloc:.2f} GB, reserved={reserv:.2f} GB")
            else:
                print(f"[CUDA] Cleared {note}")
        except Exception:
            print(f"[CUDA] Cleared {note}")

# >>> UPDATE START
def configure_cpu_threading(blas_threads: Optional[int] = None,
                            faiss_threads: Optional[int] = None):
    """
    - Sets FAISS OpenMP threads (affects `index.search` etc.)
    - Caps/sets BLAS threadpools for NumPy/Scipy (OpenBLAS/MKL/BLIS/NumExpr)
    """
    if faiss_threads:
        try:
            faiss.omp_set_num_threads(int(faiss_threads))
            print(f"[Threads] FAISS OMP threads = {int(faiss_threads)}")
        except Exception as e:
            print(f"[Threads] Failed to set FAISS threads: {e}")

    if blas_threads:
        n = int(blas_threads)
        # env vars for common BLAS backends (best-effort)
        for var in ("OMP_NUM_THREADS", "OPENBLAS_NUM_THREADS", "MKL_NUM_THREADS",
                    "BLIS_NUM_THREADS", "NUMEXPR_NUM_THREADS"):
            os.environ[var] = str(n)
        if threadpool_limits:
            try:
                # Note: threadpool_limits is a context manager; for "global" behavior,
                # we’ll wrap our heavy sections in a `with` block (see below).
                print(f"[Threads] BLAS thread cap requested = {n}")
            except Exception as e:
                print(f"[Threads] threadpool_limits not available: {e}")
# <<< UPDATE END

def model_roundtrip_cpu_gpu(model):
    if not torch.cuda.is_available():
        return model
    try:
        model.to("cpu")
    except Exception:
        pass
    clear_cuda_memory("after model->CPU")
    try:
        model.to("cuda")
    except Exception:
        pass
    clear_cuda_memory("after model->CUDA")
    return model

# -------------------- Embedding model --------------------
def device() -> str:
    return "cuda" if torch.cuda.is_available() else "cpu"

def load_model(model_name: str) -> SentenceTransformer:
    return SentenceTransformer(model_name, device=device())

def embed_texts(model: SentenceTransformer,
                texts: List[str],
                batch_size: int = 1024,
                normalize: bool = True) -> np.ndarray:
    """OOM-resilient embedding with batch backoff."""
    if not texts:
        return np.zeros((0, model.get_sentence_embedding_dimension()), dtype=np.float32)
    chunks: List[np.ndarray] = []
    i, bs = 0, max(1, int(batch_size))
    with torch.inference_mode():
        while i < len(texts):
            j = min(i + bs, len(texts))
            try:
                vecs = model.encode(
                    texts[i:j],
                    batch_size=bs,
                    convert_to_numpy=True,
                    normalize_embeddings=normalize,
                    show_progress_bar=False,
                )
                if vecs.dtype != np.float32:
                    vecs = vecs.astype(np.float32, copy=False)
                chunks.append(vecs)
                i = j
            except torch.cuda.OutOfMemoryError:
                clear_cuda_memory("OOM during embed; backoff")
                if bs == 1:
                    raise
                bs = max(1, bs // 2)
    return np.vstack(chunks)

# -------------------- Index builders --------------------
def build_hnsw(d: int, m: int = 32, efc: int = 200) -> faiss.Index:
    index = faiss.IndexHNSWFlat(d, m, faiss.METRIC_INNER_PRODUCT)
    faiss.downcast_index(index).hnsw.efConstruction = efc
    return index

def build_flat(d: int) -> faiss.Index:
    return faiss.IndexFlatIP(d)

def build_ivfpq(d: int, nlist: int = 16384, pq_m: int = 64, pq_nbits: int = 8) -> faiss.Index:
    quant = faiss.IndexFlatIP(d)
    return faiss.IndexIVFPQ(quant, d, nlist, pq_m, pq_nbits, faiss.METRIC_INNER_PRODUCT)

def choose_system(entity_type: str) -> str:
    return RXNORM if str(entity_type).strip().lower() == "medicine" else SNOMED

# -------------------- Catalog prep --------------------
def prepare_catalog(df: pd.DataFrame, system_name: str) -> pd.DataFrame:
    keep_cols = ["CUI", "System", "TTY", "CODE", "STR", "STY"]
    df = df.loc[:, keep_cols].copy()
    df["System"] = system_name
    df["CODE"] = df["CODE"].astype(str)
    df["STR"]  = df["STR"].astype(str)
    df = df.reset_index(drop=True)
    df["row_id"] = df.index.astype(np.int64)
    return df[["row_id", "CODE", "STR", "CUI", "TTY", "STY", "System"]]

def train_ivfpq_if_needed(index: faiss.Index, vecs_for_training: np.ndarray):
    inner = index
    if isinstance(index, faiss.IndexIDMap2):
        inner = index.index
    if isinstance(inner, faiss.IndexIVFPQ):
        inner.train(vecs_for_training)

# -------------------- LLM backends (vLLM / llama.cpp) --------------------
# @dataclass
# class LLMConfig:
#     backend: str = "auto"  # auto|vllm|llama-cpp
#     hf_model_id: str = UNSLOTH_VLLM_OSS_20B  # for vLLM
#     gguf_path: Optional[str] = None  # for llama-cpp
#     max_new_tokens: int = 512
#     temperature: float = 0.1
#     top_p: float = 0.9

@dataclass
class LLMConfig:
    backend: str = "auto"      # auto|vllm|llama-cpp
    hf_model_id: Optional[str] = UNSLOTH_VLLM_OSS_20B
    gguf_path: Optional[str] = None
    max_new_tokens: int = 512
    temperature: float = 0.1
    top_p: float = 0.9
    vllm_quantization: Optional[str] = None
    tp: Optional[int] = None    # tensor parallel
    n_threads: Optional[int] = None
    concurrency: Optional[int] = None

class AsyncLLMClient:
    def __init__(self, cfg: LLMConfig):
        self.cfg = cfg
        self.mode = None
        self.engine = None
        self.sampling = None
        self.executor: Optional[concurrent.futures.ThreadPoolExecutor] = None
        self._lock = asyncio.Lock()  # safety for llama shared context
        self._limiter = asyncio.BoundedSemaphore(int(self.cfg.concurrency or default_concurrency()))
        self._init()

    def _auto_defaults(self):
        if self.cfg.concurrency is None:
            self.cfg.concurrency = default_concurrency()
        if self.cfg.tp in (None, "auto"):
            self.cfg.tp = torch.cuda.device_count() if torch.cuda.is_available() else 1
            if self.cfg.tp < 1:
                self.cfg.tp = 1
        if self.cfg.n_threads in (None, "auto"):
            self.cfg.n_threads = max(1, multiprocessing.cpu_count() - 1)
    
    def _init(self):
        self._auto_defaults()
        backend, hf_id, gguf = pick_llm_backend_and_model(self.cfg.backend,
                                                          self.cfg.hf_model_id,
                                                          self.cfg.gguf_path)
        self.cfg.backend, self.cfg.hf_model_id, self.cfg.gguf_path = backend, hf_id, gguf

        if backend == "vllm":
            from vllm import SamplingParams
            from vllm.engine.arg_utils import AsyncEngineArgs
            from vllm.engine.async_llm_engine import AsyncLLMEngine

            # Hard-cap TP to visible GPUs
            tp = int(self.cfg.tp or 1)
            if torch.cuda.is_available():
                tp = max(1, min(tp, torch.cuda.device_count()))
            else:
                tp = 1

            # Default quant = bitsandbytes unless overridden
            quant = self.cfg.vllm_quantization
            if quant in (None, "", "auto"):
                is_qwen4b = (self.cfg.hf_model_id == UNSLOTH_VLLM_QWEN4B)
                quant = DEFAULT_VLLM_QUANT if is_qwen4b else DEFAULT_VLLM_QUANT_QWEN4B
            self.cfg.vllm_quantization = quant

            if self.cfg.gguf_path:
                print(f"[LLM {datetime.now().strftime(DATE_FORMAT_STR)}] Note: gguf_path is ignored on vLLM (use llama-cpp for GGUF).")

            if quant is not None:
                eng_args = AsyncEngineArgs(
                    model=self.cfg.hf_model_id or UNSLOTH_VLLM_OSS_20B,
                    dtype="auto",
                    trust_remote_code=True,
                    tensor_parallel_size=tp,
                    disable_log_stats=True,
                    quantization=quant,
                    gpu_memory_utilization=0.9,
                    max_num_seqs=2048,
                    enable_prefix_caching=True,          # huge win given identical system prompt
                    enforce_eager=True,                 # avoid lazy init stalls
                )
            
            else:
                eng_args = AsyncEngineArgs(
                    model=self.cfg.hf_model_id or UNSLOTH_VLLM_OSS_20B,
                    dtype="auto",
                    trust_remote_code=True,
                    tensor_parallel_size=tp,
                    disable_log_stats=True,
                    gpu_memory_utilization=0.9,
                    max_num_seqs=2048,
                    enable_prefix_caching=True,          # huge win given identical system prompt
                    enforce_eager=True,                 # avoid lazy init stalls
                )

            # Optional: hint for FlashInfer
            try:
                import flashinfer  # noqa: F401
                print(f"[LLM {datetime.now().strftime(DATE_FORMAT_STR)}] FlashInfer detected.")
            except Exception:
                print(f"[LLM {datetime.now().strftime(DATE_FORMAT_STR)}] FlashInfer not detected. For best perf: `pip install 'vllm[flashinfer]'`")

            self.engine = AsyncLLMEngine.from_engine_args(eng_args)
            self.sampling = SamplingParams(
                max_tokens=self.cfg.max_new_tokens,
                temperature=self.cfg.temperature,
                top_p=self.cfg.top_p,
            )
            self.mode = "vllm"
            print(f"[LLM {datetime.now().strftime(DATE_FORMAT_STR)}] vLLM ready. model={eng_args.model} tp={tp} quant={quant}")
        else:
            # llama-cpp
            from llama_cpp import Llama
            self.engine = Llama(model_path=self.cfg.gguf_path,
                                n_ctx=131072,  # large ctx; adjust if needed
                                n_threads=int(self.cfg.n_threads),
                                logits_all=False,
                                verbose=False)
            self.executor = concurrent.futures.ThreadPoolExecutor(
                max_workers=int(self.cfg.concurrency))
            self.mode = "llama-cpp"
            print(f"[LLM {datetime.now().strftime(DATE_FORMAT_STR)}] llama-cpp ready. gguf={self.cfg.gguf_path} threads={self.cfg.n_threads} "
                  f"pool={self.cfg.concurrency}")

    async def generate_one(self, system_prompt: str, user_prompt: str) -> str:
        async with self._limiter:
            if self.mode == "vllm":
                prompt = (
                    f"<|system|>\n{system_prompt}\n</s>\n"
                    f"<|user|>\n{user_prompt}\n</s>\n"
                    f"<|assistant|>\n"
                )
                request_id = str(uuid.uuid4())

                nlc = '\n'
                req = self.engine.generate(prompt, self.sampling, request_id)

                # vLLM V1 returns an async generator (stream). Older wrappers may return an awaitable or a value.
                try:
                    if inspect.isasyncgen(req):
                        last = None
                        async for out in req:        # consume stream to completion
                            last = out
                        # print(f"[LLM {datetime.now().strftime(DATE_FORMAT_STR)}] recieved output for req {request_id}: {last.outputs[0].text.strip().replace(nlc,' ')[:100] if last and last.outputs else ''}")
                        return last.outputs[0].text if last and last.outputs else ""
                    elif inspect.isawaitable(req):
                        out = await req
                        # print(f"[LLM {datetime.now().strftime(DATE_FORMAT_STR)}] recieved output for req {request_id}: {out.outputs[0].text.strip().replace(nlc,' ')[:100] if out and out.outputs else ''}")
                        return out.outputs[0].text if out and out.outputs else ""
                    else:
                        out = req
                        # print(f"[LLM {datetime.now().strftime(DATE_FORMAT_STR)}] recieved output for req {request_id}: {out.outputs[0].text.strip().replace(nlc,' ')[:100] if out and out.outputs else ''}")
                        return out.outputs[0].text if out and out.outputs else ""
                except Exception:
                    print(f"[LLM {datetime.now().strftime(DATE_FORMAT_STR)}] recieved exception for req {request_id}.")
                    # optional: log here
                    return ""
            else:
                # llama-cpp is blocking; run in thread pool
                msgs = [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ]
                loop = asyncio.get_running_loop()
                async with self._lock:  # avoid unsafe concurrent access
                    return await loop.run_in_executor(
                        self.executor,
                        lambda: self.engine.create_chat_completion(
                            messages=msgs,
                            temperature=self.cfg.temperature,
                            top_p=self.cfg.top_p,
                            max_tokens=self.cfg.max_new_tokens,
                        )["choices"][0]["message"]["content"]
                    )

    async def generate_many(self, pairs: List[Tuple[str, str]]) -> List[str]:
        # throttle via semaphore
        return await asyncio.gather(*(self.generate_one(sp, up) for sp, up in pairs))

class LLMClient:
    def __init__(self, cfg: LLMConfig):
        self.cfg = cfg
        self.impl = None
        self.mode = None
        self.SamplingParams = None
        self._init()

    def _auto_defaults(self):
        if self.cfg.tp in (None, "auto"):
            self.cfg.tp = torch.cuda.device_count() if torch.cuda.is_available() else 1
            if self.cfg.tp < 1:
                self.cfg.tp = 1
        if self.cfg.n_threads in (None, "auto"):
            self.cfg.n_threads = max(1, multiprocessing.cpu_count() - 1)
        if self.cfg.vllm_quantization in (None, "", "auto"):
            is_qwen4b = (self.cfg.hf_model_id == UNSLOTH_VLLM_QWEN4B)
            self.cfg.vllm_quantization = DEFAULT_VLLM_QUANT if not is_qwen4b else DEFAULT_VLLM_QUANT_QWEN4B

    def _init(self):
        backend, hf_id, gguf = pick_llm_backend_and_model(
            self.cfg.backend, self.cfg.hf_model_id, self.cfg.gguf_path
        )
        self.cfg.backend, self.cfg.hf_model_id, self.cfg.gguf_path = backend, hf_id, gguf

        self._auto_defaults()

        if backend == "vllm":
            from vllm import LLM as VLLM, SamplingParams
            tp = int(self.cfg.tp or 1)
            if torch.cuda.is_available():
                tp = max(1, min(tp, torch.cuda.device_count()))
            if self.cfg.gguf_path:
                print(f"[LLM {datetime.now().strftime(DATE_FORMAT_STR)}] Note: gguf_path is ignored on vLLM (use llama-cpp for GGUF).")
            self.SamplingParams = SamplingParams
            self.impl = VLLM(
                model=self.cfg.hf_model_id,
                trust_remote_code=True,
                tensor_parallel_size=tp,
                quantization=self.cfg.vllm_quantization,
            )
            self.mode = "vllm"
            print(f"[LLM {datetime.now().strftime(DATE_FORMAT_STR)}] Using vLLM. model={self.cfg.hf_model_id} tp={tp} quant={self.cfg.vllm_quantization}")
        else:
            # llama-cpp (CPU or forced)
            from llama_cpp import Llama
            if not self.cfg.gguf_path:
                self.cfg.gguf_path = hf_hub_download(
                    repo_id=UNSLOTH_GGUF_QWEN_REPO, filename=UNSLOTH_GGUF_QWEN_FILE
                )
            self.impl = Llama(
                model_path=self.cfg.gguf_path,
                n_ctx=131072,
                n_threads=int(self.cfg.n_threads),
                logits_all=False,
                verbose=False,
            )
            self.mode = "llama-cpp"
            print(f"[LLM {datetime.now().strftime(DATE_FORMAT_STR)}] Using llama-cpp. gguf={self.cfg.gguf_path} threads={self.cfg.n_threads}")

    def generate(self, system_prompt: str, user_prompt: str) -> str:
        if self.mode == "vllm":
            sp = self.SamplingParams(
                max_tokens=self.cfg.max_new_tokens,
                temperature=self.cfg.temperature,
                top_p=self.cfg.top_p,
            )
            prompt = f"<|system|>\n{system_prompt}\n</s>\n<|user|>\n{user_prompt}\n</s>\n<|assistant|>\n"
            outs = self.impl.generate(prompt, sp)
            return outs[0].outputs[0].text
        else:
            msgs = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ]
            out = self.impl.create_chat_completion(
                messages=msgs,
                temperature=self.cfg.temperature,
                top_p=self.cfg.top_p,
                max_tokens=self.cfg.max_new_tokens,
            )
            return out["choices"][0]["message"]["content"]

# -------------------- LLM prompts & XML parsing --------------------
EXPAND_SYSTEM_PROMPT = """You are an expert clinical terminology normalizer. 
Task: normalize messy clinical inputs into standardized concepts for mapping to RxNorm (medications) and SNOMED CT (labs, diagnoses, procedures).
Inputs may be abbreviated, colloquial, misspelled, or non-standard (e.g., "paracetamol" vs "Acetaminophen", "xr chest" vs "X-ray chest").
Return results STRICTLY as XML with one or more <candidate> blocks.
Each <candidate> block describes a distinct possible interpretation of the input.
It is for the case, when you are not sure about the exact meaning, to cover multiple plausible interpretations.
It is optional to give multiple candidates, if you are sure about the meaning, a single candidate is fine.
But if you are unsure, give multiple candidates to cover the possibilities (ideally 2-3, but upto 5 if needed).
For each candidate, include:
  <alternate_keywords>comma-separated short terms and brand/common names along with scientific names (give 2-5, and at max 10 alternate keywords)</alternate_keywords>
  <description>clear medical explanation (1-3 sentences)</description>
  <possible_semantic_term_types>comma-separated choices FROM THE PROVIDED LIST ONLY</possible_semantic_term_types>
Do not add any other tags. Do not include markdown. Use only ASCII punctuation.
Semantic type choices (union of RxNorm & SNOMED CT types):
{sty_inventory}
"""

EXPAND_USER_PROMPT = """Input Entity Description: {text}
Entity Type: {entity_type}

Output the XML now.
"""

RERANK_SYSTEM_PROMPT = """You are ranking standardized clinical codes for a messy input. 
You receive the original input & entity type, plus up to 50 candidate codes (from RxNorm and SNOMED CT) with strings and types.
Goal: select the BEST top-N codes consistent with the input meaning and entity type.
Guidance:
- Prefer RxNorm for medications; prefer SNOMED CT for labs/diagnoses/procedures. 
- However, if preferred coding system is missing or clearly worse, it's OK to choose the other system.
- Consider code string (STR), term type (TTY), semantic type (STY), and overall plausibility.
Return STRICT XML: a sequence of <choice> blocks; each contains:
  <code>THE EXACT CODE STRING</code>
  <reasoning>concise justification</reasoning>
Do not include other tags, no markdown, no extra text.
"""

RERANK_USER_PROMPT = """Original Query: {text}
Entity Type: {entity_type}

Expanded understanding (from earlier step):
{expanded_summary}

Candidate codes (system | code | STY | TTY | one or more names):
{candidates_blob}

Return the XML with up to {final_k} <choice> items.
"""

def extract_xml_candidates(xml_text: str) -> List[Dict[str, Any]]:
    """
    Parse <candidate> blocks from expansion XML.
    Robust to minor formatting issues; falls back to regex if needed.
    """
    # Simple normalization
    s = xml_text.strip()
    # Greedy but robust: split by <candidate>...</candidate>
    blocks = re.findall(r"<candidate>(.*?)</candidate>", s, flags=re.DOTALL | re.IGNORECASE)
    if not blocks:
        # Try single-block format without outer tags
        blocks = [s]
    results = []
    for b in blocks:
        def grab(tag):
            m = re.search(rf"<{tag}>(.*?)</{tag}>", b, flags=re.DOTALL | re.IGNORECASE)
            return m.group(1).strip() if m else ""
        kws = grab("alternate_keywords")
        desc = grab("description")
        stys = grab("possible_semantic_term_types")
        kw_list = [k.strip() for k in re.split(r"[,\n;]", kws) if k.strip()]
        sty_list = [k.strip() for k in re.split(r"[,\n;]", stys) if k.strip()]
        if not kw_list and not desc and not sty_list:
            continue
        results.append({
            "alternate_keywords": kw_list,
            "description": desc,
            "possible_stys": sty_list
        })
    if not results:
        # Fallback: treat entire text as one candidate description
        results = [{"alternate_keywords": [], "description": s[:400], "possible_stys": []}]
    return results

def extract_xml_choices(xml_text: str) -> List[Dict[str, str]]:
    """Parse <choice><code>…</code><reasoning>…</reasoning></choice>"""
    blocks = re.findall(r"<choice>(.*?)</choice>", xml_text, flags=re.DOTALL | re.IGNORECASE)
    out = []
    for b in blocks:
        code = re.search(r"<code>(.*?)</code>", b, flags=re.DOTALL | re.IGNORECASE)
        reas = re.search(r"<reasoning>(.*?)</reasoning>", b, flags=re.DOTALL | re.IGNORECASE)
        if code:
            out.append({"code": code.group(1).strip(), "reasoning": (reas.group(1).strip() if reas else "")})
    return out

async def llm_expand_query_async(llm: AsyncLLMClient, text: str, entity_type: str, sty_inventory: List[str]) -> List[Dict]:
    sys_prompt = EXPAND_SYSTEM_PROMPT.format(sty_inventory="\n".join(f"- {s}" for s in sty_inventory))
    user_prompt = EXPAND_USER_PROMPT.format(text=text, entity_type=entity_type)
    nlc = '\n'
    print(f"[LLM {datetime.now().strftime(DATE_FORMAT_STR)}] Expanding query for: {text[:60].replace(nlc, ' ')}...")
    out = await llm.generate_one(sys_prompt, user_prompt)
    return extract_xml_candidates(out)

async def llm_rerank_codes_async(llm: AsyncLLMClient, text: str, entity_type: str,
                                 expanded_summary: str, candidates_blob: str, final_k: int) -> List[str]:
    sys_prompt = RERANK_SYSTEM_PROMPT
    user_prompt = RERANK_USER_PROMPT.format(text=text, entity_type=entity_type,
                                            expanded_summary=expanded_summary,
                                            candidates_blob=candidates_blob,
                                            final_k=final_k)
    out = await llm.generate_one(sys_prompt, user_prompt)
    return [c["code"] for c in extract_xml_choices(out)][:final_k]

# -------------------- BUILD --------------------
def build_indices(args):
    maybe_hf_login(args.hf_token)

    os.makedirs(args.out_dir, exist_ok=True)
    model = load_model(args.model)
    dim = model.get_sentence_embedding_dimension()
    print(f"[Build] Embedding model: {args.model} (dim={dim}, device={device()})")

    snomed_df = pd.read_parquet(args.snomed_parquet)
    rxnorm_df = pd.read_parquet(args.rxnorm_parquet)

    snomed_cat = prepare_catalog(snomed_df, SNOMED)
    rxnorm_cat = prepare_catalog(rxnorm_df, RXNORM)

    snomed_cat.to_parquet(os.path.join(args.out_dir, SNOMED_CAT_NAME), index=False)
    rxnorm_cat.to_parquet(os.path.join(args.out_dir, RXNORM_CAT_NAME), index=False)

    sn_vec_path = os.path.join(args.out_dir, "snomed_vectors.f32")
    rx_vec_path = os.path.join(args.out_dir, "rxnorm_vectors.f32")
    sn_vec_mm = np.memmap(sn_vec_path, dtype=np.float32, mode="w+",
                          shape=(len(snomed_cat), dim))
    rx_vec_mm = np.memmap(rx_vec_path, dtype=np.float32, mode="w+",
                          shape=(len(rxnorm_cat), dim))

    # Build indices
    def make_index() -> faiss.Index:
        it = args.index.lower()
        if it == "hnsw":
            return build_hnsw(dim, m=args.hnsw_m, efc=args.hnsw_efc)
        elif it == "ivfpq":
            return build_ivfpq(dim, nlist=args.ivf_nlist, pq_m=args.pq_m, pq_nbits=args.pq_nbits)
        elif it == "flat":
            return build_flat(dim)
        raise ValueError("--index must be: hnsw | ivfpq | flat")

    # --- SNOMED ---
    print(f"[Build] SNOMED rows: {len(snomed_cat):,}")
    sn_index = make_index()
    if args.index.lower() == "ivfpq":
        sample_sz = min(200_000, len(snomed_cat))
        sample_idx = np.linspace(0, len(snomed_cat) - 1, num=sample_sz, dtype=int)
        sample_vecs = embed_texts(model, snomed_cat.loc[sample_idx, "STR"].tolist(), batch_size=args.batch)
        train_ivfpq_if_needed(sn_index, sample_vecs)
    sn_index = faiss.IndexIDMap2(sn_index)
    for start in tqdm(range(0, len(snomed_cat), args.batch), desc="[Build] SNOMED add"):
        end = min(start + args.batch, len(snomed_cat))
        vecs = embed_texts(model, snomed_cat.loc[start:end-1, "STR"].tolist(), batch_size=args.batch)
        ids  = snomed_cat.loc[start:end-1, "row_id"].to_numpy(dtype=np.int64)
        sn_index.add_with_ids(vecs, ids)
        sn_vec_mm[ids] = vecs
    if args.index.lower() == "hnsw":
        faiss.downcast_index(sn_index.index).hnsw.efSearch = args.hnsw_efs
    faiss.write_index(sn_index, os.path.join(args.out_dir, SNOMED_INDEX_NAME))
    print("[Build] Saved SNOMED index.")

    # --- Barrier ---
    try:
        del vecs, ids
    except Exception:
        pass
    clear_cuda_memory("after SNOMED phase")
    model = model_roundtrip_cpu_gpu(model)

    # --- RXNORM ---
    print(f"[Build] RxNorm rows: {len(rxnorm_cat):,}")
    rx_index = make_index()
    if args.index.lower() == "ivfpq":
        sample_sz = min(200_000, len(rxnorm_cat))
        sample_idx = np.linspace(0, len(rxnorm_cat) - 1, num=sample_sz, dtype=int)
        sample_vecs = embed_texts(model, rxnorm_cat.loc[sample_idx, "STR"].tolist(), batch_size=args.batch)
        train_ivfpq_if_needed(rx_index, sample_vecs)
    rx_index = faiss.IndexIDMap2(rx_index)
    for start in tqdm(range(0, len(rxnorm_cat), args.batch), desc="[Build] RxNorm add"):
        end = min(start + args.batch, len(rxnorm_cat))
        vecs = embed_texts(model, rxnorm_cat.loc[start:end-1, "STR"].tolist(), batch_size=args.batch)
        ids  = rxnorm_cat.loc[start:end-1, "row_id"].to_numpy(dtype=np.int64)
        rx_index.add_with_ids(vecs, ids)
        rx_vec_mm[ids] = vecs
    if args.index.lower() == "hnsw":
        faiss.downcast_index(rx_index.index).hnsw.efSearch = args.hnsw_efs
    faiss.write_index(rx_index, os.path.join(args.out_dir, RXNORM_INDEX_NAME))
    print("[Build] Saved RxNorm index.")

    sn_vec_mm.flush()
    rx_vec_mm.flush()

    # --- STY vocabulary embeddings for fast STY similarity ---
    sty_vocab = sorted(list(set(snomed_cat["STY"].dropna().astype(str)) | set(rxnorm_cat["STY"].dropna().astype(str))))
    np.save(os.path.join(args.out_dir, STY_EMB_NPY),
            embed_texts(model, sty_vocab, batch_size=min(512, args.batch), normalize=True))
    with open(os.path.join(args.out_dir, STY_VOCAB_JSON), "w", encoding="utf-8") as f:
        json.dump({"sty_vocab": sty_vocab}, f, indent=2)

    meta = {
        "model": args.model,
        "dim": dim,
        "index_type": args.index.lower(),
        "metric": "ip_cosine_normalized",
        "hnsw": {"M": args.hnsw_m, "efConstruction": args.hnsw_efc, "efSearch": args.hnsw_efs} if args.index.lower()=="hnsw" else None,
        "ivfpq": {"nlist": args.ivf_nlist, "pq_m": args.pq_m, "pq_nbits": args.pq_nbits} if args.index.lower()=="ivfpq" else None,
        "batch": args.batch,
        "files": {
            "snomed_index": SNOMED_INDEX_NAME,
            "rxnorm_index": RXNORM_INDEX_NAME,
            "snomed_catalog": SNOMED_CAT_NAME,
            "rxnorm_catalog": RXNORM_CAT_NAME,
            "sty_vocab_json": STY_VOCAB_JSON,
            "sty_embeddings": STY_EMB_NPY,
            "snomed_vectors": "snomed_vectors.f32",
            "rxnorm_vectors": "rxnorm_vectors.f32",
        }
    }
    with open(os.path.join(args.out_dir, META_NAME), "w", encoding="utf-8") as f:
        json.dump(meta, f, indent=2)
    print(f"[Build] Done. Artifacts in: {args.out_dir}")

# -------------------- LOAD --------------------
def load_bundle(index_dir: str):
    with open(os.path.join(index_dir, META_NAME), "r", encoding="utf-8") as f:
        meta = json.load(f)
    sn_index = faiss.read_index(os.path.join(index_dir, meta["files"]["snomed_index"]))
    rx_index = faiss.read_index(os.path.join(index_dir, meta["files"]["rxnorm_index"]))
    sn_cat = pd.read_parquet(os.path.join(index_dir, meta["files"]["snomed_catalog"]))
    rx_cat = pd.read_parquet(os.path.join(index_dir, meta["files"]["rxnorm_catalog"]))
    with open(os.path.join(index_dir, meta["files"]["sty_vocab_json"]), "r", encoding="utf-8") as f:
        sty_vocab = json.load(f)["sty_vocab"]
    sty_emb = np.load(os.path.join(index_dir, meta["files"]["sty_embeddings"]))
    model = load_model(meta["model"])
    sn_vecs = np.memmap(os.path.join(index_dir, meta["files"]["snomed_vectors"]),
                        dtype=np.float32, mode="r", shape=(len(sn_cat), meta["dim"]))
    rx_vecs = np.memmap(os.path.join(index_dir, meta["files"]["rxnorm_vectors"]),
                        dtype=np.float32, mode="r", shape=(len(rx_cat), meta["dim"]))
    return {
        "meta": meta,
        "model": model,
        "snomed": {"index": sn_index, "catalog": sn_cat, "vecs": sn_vecs},
        "rxnorm": {"index": rx_index, "catalog": rx_cat, "vecs": rx_vecs},
        "sty": {"vocab": sty_vocab, "emb": sty_emb}
    }

# -------------------- Utilities: normalize cos to [0,1] --------------------
def cos_to_01(x: float) -> float:
    return max(0.0, min(1.0, 0.5 * (x + 1.0)))

# -------------------- Scoring building blocks --------------------
def faiss_search(index: faiss.Index, vecs: np.ndarray, topk: int) -> Tuple[np.ndarray, np.ndarray]:
    return index.search(vecs, topk)

def reconstruct_vec(index: faiss.Index, row_id: int, d: int) -> Optional[np.ndarray]:
    try:
        v = np.empty(d, dtype=np.float32)
        index.reconstruct(row_id, v)
        # ensure normalized (FAISS stored were normalized)
        n = np.linalg.norm(v)
        if n > 0:
            v = v / n
        return v
    except Exception:
        return None

# Build hits: turn FAISS (D,I) to list of rows with metadata & score
def build_row_hits(D_row: np.ndarray, I_row: np.ndarray, catalog: pd.DataFrame) -> List[Dict]:
    hits = []
    for idx, score in zip(I_row, D_row):
        idx = int(idx)
        if idx == -1: 
            continue
        row = catalog.loc[catalog["row_id"] == idx].iloc[0]
        hits.append({
            "row_id": idx,
            "CODE": str(row["CODE"]),
            "System": row["System"],
            "STR": row["STR"],
            "STY": row["STY"],
            "TTY": row["TTY"],
            "score": float(score)
        })
    return hits

# -------------------- ADVANCED PIPELINE (per-query) --------------------
@dataclass
class AdvancedWeights:
    w_desc: float = 0.30
    w_kw:   float = 0.40
    w_direct: float = 0.20
    w_sty:  float = 0.10

@dataclass
class AdvancedParams:
    semantic_top_desc: int = 500
    semantic_top_direct: int = 500
    semantic_top_kw_each: int = 100
    fuzzy_pool: int = 500
    top_pool_after_fuzzy: int = 250
    final_llm_candidates: int = 30
    final_output_k: int = 10
    fuzzy_workers: int = -1   # RapidFuzz cdist workers (-1 = all cores)

def llm_expand_query(llm: LLMClient, text: str, entity_type: str, sty_inventory: List[str]) -> List[Dict]:
    sys_prompt = EXPAND_SYSTEM_PROMPT.format(sty_inventory="\n".join(f"- {s}" for s in sty_inventory))
    user_prompt = EXPAND_USER_PROMPT.format(text=text, entity_type=entity_type)
    out = llm.generate(sys_prompt, user_prompt)
    return extract_xml_candidates(out)

def llm_rerank_codes(llm: LLMClient,
                     text: str,
                     entity_type: str,
                     expanded_summary: str,
                     candidates_blob: str,
                     final_k: int) -> List[str]:
    sys_prompt = RERANK_SYSTEM_PROMPT
    user_prompt = RERANK_USER_PROMPT.format(text=text, entity_type=entity_type,
                                            expanded_summary=expanded_summary,
                                            candidates_blob=candidates_blob,
                                            final_k=final_k)
    out = llm.generate(sys_prompt, user_prompt)
    choices = extract_xml_choices(out)
    return [c["code"] for c in choices][:final_k]

def build_expanded_summary(cands: List[Dict]) -> str:
    lines = []
    for i, c in enumerate(cands, 1):
        ak = ", ".join(c.get("alternate_keywords", [])[:10])
        stys = ", ".join(c.get("possible_stys", [])[:8])
        desc = c.get("description", "")
        lines.append(f"[Candidate {i}] AK: {ak}\nSTY: {stys}\nDESC: {desc}")
    return "\n".join(lines)

def precompute_sty_scores_map(pred_stys: List[str],
                              sty_vocab: List[str],
                              sty_emb: np.ndarray) -> Dict[str, float]:
    """
    Return a dict: candidate STY string -> [0,1] score,
    computed as max cosine(sim(pred_stys), cand_sty) using the prebuilt sty_emb.
    Assumes sty_emb rows are already L2-normalized.
    """
    if not pred_stys:
        return {}

    sty2idx = {s: i for i, s in enumerate(sty_vocab)}
    idxs = [sty2idx[s] for s in pred_stys if s in sty2idx]
    if not idxs:
        return {}

    P = sty_emb[idxs]                 # (m, d)
    # max over predicted STYs against all vocab vectors in one shot
    # result shape: (|vocab|,)
    max_sims = (P @ sty_emb.T).max(axis=0)
    # map to [0,1] and return as dict
    return {sty_vocab[j]: cos_to_01(float(max_sims[j])) for j in range(len(sty_vocab))}

def compute_sty_score(pred_stys: List[str], sty_vocab: List[str], sty_emb: np.ndarray,
                      model: SentenceTransformer, candidate_sty: str) -> float:
    if not pred_stys or not candidate_sty:
        return 0.0
    # map candidate sty embedding
    try:
        idx = sty_vocab.index(candidate_sty)
        cand_vec = sty_emb[idx:idx+1]  # (1,d)
    except ValueError:
        return 0.0
    # embed predicted list and take max similarity
    pred_vecs = embed_texts(model, pred_stys, batch_size=min(64, len(pred_stys)))
    pred_vecs = pred_vecs / np.linalg.norm(pred_vecs, axis=1, keepdims=True).clip(min=1e-9)
    cand_vec = cand_vec / np.linalg.norm(cand_vec, axis=1, keepdims=True).clip(min=1e-9)
    sims = (pred_vecs @ cand_vec.T).flatten()
    return cos_to_01(float(np.max(sims))) if sims.size else 0.0

def fuzzy_score_for_str(s: str, query: str, alt_keywords: List[str]) -> float:
    if not HAVE_RAPIDFUZZ:
        return 0.0
    best = fuzz.token_set_ratio(query, s) / 100.0
    for kw in alt_keywords:
        best = max(best, fuzz.token_set_ratio(kw, s) / 100.0)
    return best

# --- Fast fuzzy scoring (batched + parallel) ---
def _rf_norm(s: str) -> str:
    try:
        from rapidfuzz.utils import default_process
        return default_process(s or "")
    except Exception:
        return re.sub(r"\W+", " ", str(s).lower()).strip()

def fuzzy_scores_max(query: str,
                     alt_keywords: List[str],
                     choices: List[str],
                     kw_limit: int = 8,
                     workers: int = -1,
                     prefilter: int = 200,
                     use_two_stage: bool = True) -> np.ndarray:
    """
    Return an array of max fuzzy scores in [0,1] for each choice string,
    computed against {query} ∪ top-{kw_limit} alt keywords.

    - Batch + parallel via RapidFuzz
    - Optional two-stage: ratio prefilter -> token_set_ratio refine
    """
    if not HAVE_RAPIDFUZZ or not choices:
        return np.zeros(len(choices), dtype=np.float32)

    from rapidfuzz import process as rf_process, fuzz as rf_fuzz

    # Prepare anchors (query + limited KWs), normalized & deduped
    anchors = [_rf_norm(query)]
    for kw in alt_keywords:
        t = _rf_norm(kw)
        if len(t) < 3:            # drop trivially short tokens
            continue
        if t not in anchors:
            anchors.append(t)
        if len(anchors) >= 1 + kw_limit:
            break

    choices_proc = [_rf_norm(c) for c in choices]
    N = len(choices_proc)

    # Stage 1: quick prefilter (ratio) to shrink the set
    if use_two_stage:
        keep = np.zeros(N, dtype=bool)
        lim = min(prefilter, N)
        for a in anchors:
            # Parallel, C++ backend; 'workers' is supported here
            arr = rf_process.cdist([a], choices_proc, scorer=rf_fuzz.ratio, workers=workers)[0]  # 0..100
            # Take top 'lim' indices for this anchor
            if lim < N:
                idxs_top = np.argpartition(arr, -lim)[-lim:]
            else:
                idxs_top = np.arange(N, dtype=int)
            keep[idxs_top] = True

        idxs = np.flatnonzero(keep)
        if idxs.size == 0:
            return np.zeros(N, dtype=np.float32)

        reduced = [choices_proc[i] for i in idxs]
    else:
        idxs = np.arange(N, dtype=int)
        reduced = choices_proc

    # Stage 2: heavy scorer on reduced set (batched)
    best = np.zeros(len(reduced), dtype=np.float32)
    for a in anchors:
        arr = rf_process.cdist([a], reduced, scorer=rf_fuzz.token_set_ratio, workers=workers)[0]
        # no divide yet; keep 0..100 for precision, cast to float32
        np.maximum(best, arr.astype(np.float32), out=best)

    scores = np.zeros(N, dtype=np.float32)
    scores[idxs] = best / 100.0   # normalize to 0..1 like before
    return scores

# -------- Async wrappers for blocking compute --------
_embed_sem = None
_faiss_sem = None

def _init_sems():
    global _embed_sem, _faiss_sem
    # limit concurrent GPU-heavy work; tune to your setup
    max_gpu_jobs = torch.cuda.device_count() or 1
    emb_slots = (2 * max_gpu_jobs) if torch.cuda.is_available() else max(1, multiprocessing.cpu_count() // 2)

    # Honor shared CPU pool size if set (from run_batch runner)
    pool_cap_env = os.environ.get("CPU_POOL", "")
    try:
        pool_cap = int(pool_cap_env) if pool_cap_env else None
    except Exception:
        pool_cap = None
    if pool_cap:
        # keep some headroom so BLAS/FAISS threads can run
        emb_slots = min(emb_slots, max(1, pool_cap // 2))

    faiss_slots = max(2, emb_slots * 2)
    _embed_sem = asyncio.Semaphore(emb_slots)
    _faiss_sem = asyncio.Semaphore(faiss_slots)

async def embed_texts_async(model, texts, batch_size=1024, normalize=True):
    if _embed_sem is None:
        _init_sems()
    async with _embed_sem:
        # offload to a thread so the event loop can run other tasks
        return await asyncio.to_thread(embed_texts, model, texts, batch_size, normalize)

async def faiss_search_async(index: faiss.Index, vecs: np.ndarray, topk: int):
    if _faiss_sem is None:
        _init_sems()
    async with _faiss_sem:
        return await asyncio.to_thread(faiss_search, index, vecs, topk)

async def reconstruct_vec_async(index: faiss.Index, row_id: int, d: int):
    # cheap, but still offload to avoid blocking
    return await asyncio.to_thread(reconstruct_vec, index, row_id, d)

async def fuzzy_scores_max_async(query, alt_keywords, choices, **kwargs):
    return await asyncio.to_thread(fuzzy_scores_max, query, alt_keywords, choices, **kwargs)

def advanced_match_one(query: str,
                       entity_type: str,
                       bundle: Dict,
                       llm: Optional[LLMClient],
                       use_llm_clean: bool,
                       use_llm_rerank: bool,
                       weights: AdvancedWeights,
                       params: AdvancedParams,
                       rerank_mode: str) -> Tuple[List[Dict], str]:
    """
    Returns (final_ranked_rows, reason_string). Each row dict has System/CODE/STR/STY/TTY/score.
    Searches BOTH systems and merges. Optimized to avoid per-row STY embeddings and to batch KW searches.
    """
    model = bundle["model"]
    sn_index, sn_cat = bundle["snomed"]["index"], bundle["snomed"]["catalog"]
    rx_index, rx_cat = bundle["rxnorm"]["index"], bundle["rxnorm"]["catalog"]
    sty_vocab, sty_emb = bundle["sty"]["vocab"], bundle["sty"]["emb"]

    # ---- 1) LLM expansion ----
    if use_llm_clean and llm is not None:
        exp_cands = llm_expand_query(llm, query, entity_type, sty_vocab)
    else:
        exp_cands = [{"alternate_keywords": [], "description": "", "possible_stys": []}]
    if not exp_cands:
        exp_cands = [{"alternate_keywords": [], "description": "", "possible_stys": []}]
    exp_cands = exp_cands[:5]
    expanded_summary = build_expanded_summary(exp_cands)

    # ---- 2) Embeddings for signals ----
    q_vec = embed_texts(model, [query], batch_size=4)[0:1]

    row_scores: Dict[Tuple[str,int], Dict[str, Any]] = {}  # key=(system,row_id)

    def ensure_store(system: str, r: Dict) -> Dict[str, Any]:
        key = (system, r["row_id"])
        if key not in row_scores:
            row_scores[key] = {**r, "score_components": {}, "composite": 0.0}
        return row_scores[key]

    def search_one_system(system: str, vec: np.ndarray, topk: int) -> List[Dict]:
        idx = sn_index if system == SNOMED else rx_index
        cat = sn_cat if system == SNOMED else rx_cat
        D, I = faiss_search(idx, vec, topk)
        return build_row_hits(D[0], I[0], cat)

    def batch_search_one_system(system: str, vecs: np.ndarray, topk: int) -> List[List[Dict]]:
        if vecs.shape[0] == 0:
            return []
        idx = sn_index if system == SNOMED else rx_index
        cat = sn_cat if system == SNOMED else rx_cat
        D, I = faiss_search(idx, vecs, topk)  # vecs: (m,d)
        return [build_row_hits(D[i], I[i], cat) for i in range(len(vecs))]

    # ----- Direct query (do once) -----
    for sys in (SNOMED, RXNORM):
        hits = search_one_system(sys, q_vec, params.semantic_top_direct)
        for r in hits:
            st = ensure_store(sys, r)
            sc = st["score_components"]
            sc["direct"] = max(sc.get("direct", 0.0), cos_to_01(r["score"]))

    # For each candidate: description, alt keywords, STY map
    for cand in exp_cands:
        alt_kws: List[str] = cand.get("alternate_keywords", [])
        desc: str = cand.get("description", "")
        pred_stys: List[str] = cand.get("possible_stys", [])

        # ----- description -----
        if desc.strip():
            d_vec = embed_texts(model, [desc], batch_size=4)[0:1]
            for sys in (SNOMED, RXNORM):
                hits = search_one_system(sys, d_vec, params.semantic_top_desc)
                for r in hits:
                    st = ensure_store(sys, r)
                    sc = st["score_components"]
                    sc["desc"] = max(sc.get("desc", 0.0), cos_to_01(r["score"]))

        # ----- keywords (batched) -----
        kw_vecs = embed_texts(model, alt_kws, batch_size=min(64, len(alt_kws))) if alt_kws else np.zeros((0, q_vec.shape[1]), np.float32)
        for sys in (SNOMED, RXNORM):
            batched = batch_search_one_system(sys, kw_vecs, params.semantic_top_kw_each)
            for hits in batched:
                for r in hits:
                    st = ensure_store(sys, r)
                    sc = st["score_components"]
                    sc["kw"] = max(sc.get("kw", 0.0), cos_to_01(r["score"]))

        # ----- STY similarity (precomputed map; take max across candidates) -----
        if pred_stys:
            sty_map = precompute_sty_scores_map(pred_stys, sty_vocab, sty_emb)
            for st in row_scores.values():
                prev = st["score_components"].get("sty", 0.0)
                st["score_components"]["sty"] = max(prev, sty_map.get(st.get("STY", ""), 0.0))

    # Fill missing components & composite
    for st in row_scores.values():
        sc = st["score_components"]
        sc.setdefault("desc", 0.0)
        sc.setdefault("kw", 0.0)
        sc.setdefault("direct", 0.0)
        sc.setdefault("sty", 0.0)
        st["composite"] = (weights.w_desc * sc["desc"] +
                           weights.w_kw   * sc["kw"] +
                           weights.w_direct * sc["direct"] +
                           weights.w_sty  * sc["sty"])

    # ---- 3) Take top pool and fuzzy re-rank ----
    pooled_rows = sorted(row_scores.values(), key=lambda x: x["composite"], reverse=True)[:params.fuzzy_pool]

    if HAVE_RAPIDFUZZ and pooled_rows:
        # Build a compact keyword bag
        all_kws = []
        for c in exp_cands:
            all_kws.extend(c.get("alternate_keywords", []))
        # Compute batched fuzzy scores (parallel)
        choices = [r["STR"] for r in pooled_rows]
        fuzzy_arr = fuzzy_scores_max(
            query=query,
            alt_keywords=list(dict.fromkeys(all_kws))[:30],   # still cap total collected
            choices=choices,
            kw_limit=8,            # tuneable; 6–10 is a good sweet spot
            workers=-1,            # all CPU cores
            prefilter=max(200, params.top_pool_after_fuzzy * 2),  # size of the reduced set
            use_two_stage=True
        )
        for r, sc in zip(pooled_rows, fuzzy_arr):
            r["fuzzy"] = float(sc)
        pooled_rows.sort(key=lambda x: x["fuzzy"], reverse=True)

    top_rows = pooled_rows[:params.top_pool_after_fuzzy]


    # ---- 4) Aggregate per code ----
    cat_by_sys = {SNOMED: sn_cat, RXNORM: rx_cat}
    idx_by_sys = {SNOMED: sn_index, RXNORM: rx_index}

    have_desc = any(c.get("description", "").strip() for c in exp_cands)
    desc_vec = embed_texts(model, [max(exp_cands, key=lambda c: len(c.get("description", "")))["description"]], 4)[0:1] if have_desc else None
    all_kws = []
    for c in exp_cands: all_kws += c.get("alternate_keywords", [])
    kw_vecs_all = embed_texts(model, list(dict.fromkeys(all_kws)), batch_size=min(64, max(1, len(all_kws)))) if all_kws else None
    pred_stys_all = []
    for c in exp_cands: pred_stys_all += c.get("possible_stys", [])
    pred_stys_all = list(dict.fromkeys(pred_stys_all))
    sty_map_all = precompute_sty_scores_map(pred_stys_all, sty_vocab, sty_emb)

    def composite_for_rowvec(r: Dict, vec: np.ndarray) -> float:
        sc_desc = float((desc_vec @ vec.reshape(1,-1).T).item()) if desc_vec is not None else 0.0
        sc_kw = float(np.max(kw_vecs_all @ vec.reshape(1,-1).T)) if (kw_vecs_all is not None and kw_vecs_all.shape[0] > 0) else 0.0
        sc_dir = float((q_vec @ vec.reshape(1,-1).T).item())
        sc_sty = sty_map_all.get(r.get("STY", ""), 0.0)
        return (weights.w_desc * cos_to_01(sc_desc) +
                weights.w_kw   * cos_to_01(sc_kw)   +
                weights.w_direct * cos_to_01(sc_dir) +
                weights.w_sty  * sc_sty)

    # Bucket rows by (system, code)
    codes: Dict[Tuple[str, str], Dict[str, Any]] = {}
    for r in top_rows:
        key = (r["System"], r["CODE"])
        codes.setdefault(key, {"rows": [], "rows_top": []})
        codes[key]["rows_top"].append(r)

    d = bundle["meta"]["dim"]
    for (system, code), bucket in codes.items():
        cat = cat_by_sys[system]
        idx = idx_by_sys[system]
        all_rows = cat.loc[cat["CODE"] == code]
        bucket["rows"] = []
        for _, row in all_rows.iterrows():
            rid = int(row["row_id"])
            vec = reconstruct_vec(idx, rid, d)
            if vec is None:
                continue
            rdict = {"row_id": rid, "System": system, "CODE": code, "STR": row["STR"], "STY": row["STY"], "TTY": row["TTY"]}
            rdict["composite"] = composite_for_rowvec(rdict, vec)
            bucket["rows"].append(rdict)

    ranked_codes = []
    for (system, code), bucket in codes.items():
        rows_all = bucket["rows"]
        rows_top = bucket["rows_top"]
        if not rows_top or not rows_all:
            continue
        avg_all = float(np.mean([r["composite"] for r in rows_all]))
        avg_500 = float(np.mean([r["composite"] for r in rows_top]))
        denom = max(1, len(rows_all))
        pct = (len(rows_top) / denom) * 100.0
        pct_safe = max(1.0001, pct)
        boost = math.sqrt(math.log10(pct_safe))
        ranked_codes.append({"System": system, "CODE": code,
                             "avg_all": avg_all, "avg_500": avg_500,
                             "pct_in_top500": pct, "final": avg_all * avg_500 * boost})

    ranked_codes.sort(key=lambda x: x["final"], reverse=True)
    topN_codes = ranked_codes[:params.final_llm_candidates]

    def best_name_for_code(system: str, code: str) -> Dict:
        cat = cat_by_sys[system]
        subset = cat.loc[cat["CODE"] == code].copy()
        subset["tty_rank"] = subset["TTY"].map({"PT": 0, "FN": 1, "SCD": 1, "SBD": 1}).fillna(5)
        win = subset.sort_values(["tty_rank"]).iloc[0]
        return {"STR": win["STR"], "STY": win["STY"], "TTY": win["TTY"]}

    topN_rows = [{**c, **best_name_for_code(c["System"], c["CODE"])} for c in topN_codes]

    if use_llm_rerank and llm is not None and len(topN_rows) > 0:
        cand_blob = "\n".join([f"{r['System']} | {r['CODE']} | {r['STY']} | {r['TTY']} | {r['STR']}" for r in topN_rows])
        final_codes = llm_rerank_codes(llm, query, entity_type, expanded_summary, cand_blob, params.final_output_k)
        best_by_code: Dict[str, Tuple[str, str, float]] = {}
        for r in topN_rows:
            tup = best_by_code.get(r["CODE"])
            if (tup is None) or (r["final"] > tup[2]):
                best_by_code[r["CODE"]] = (r["System"], r["CODE"], r.get("final", 0.0))
        final_rows = []
        for code in final_codes:
            if code in best_by_code:
                sys, c, _ = best_by_code[code]
                rep = best_name_for_code(sys, c)
                final_rows.append({"System": sys, "CODE": c, **rep})
        seen = {(r["System"], r["CODE"]) for r in final_rows}
        for r in topN_rows:
            if (r["System"], r["CODE"]) not in seen and len(final_rows) < params.final_output_k:
                final_rows.append({"System": r["System"], "CODE": r["CODE"],
                                   "STR": r["STR"], "STY": r["STY"], "TTY": r["TTY"]})
        return final_rows, "LLM rerank (RxNorm preferred for meds; SNOMED otherwise)"

    final_rows = [{"System": r["System"], "CODE": r["CODE"], "STR": r["STR"], "STY": r["STY"], "TTY": r["TTY"]}
                  for r in topN_rows[:params.final_output_k]]
    return final_rows, "Advanced scoring without LLM rerank"


async def advanced_match_one_async(query: str,
                                   entity_type: str,
                                   bundle: Dict,
                                   llm: Optional[AsyncLLMClient],
                                   use_llm_clean: bool,
                                   use_llm_rerank: bool,
                                   weights: AdvancedWeights,
                                   params: AdvancedParams,
                                   rerank_mode: str) -> Tuple[List[Dict], str]:
    """
    Returns (final_ranked_rows, reason_string). Each row dict has System/CODE/STR/STY/TTY/score.
    This function searches BOTH systems and merges.
    """

    nlc = '\n'
    print(f"[LLM {datetime.now().strftime(DATE_FORMAT_STR)}] Started matching process for: {query[:60].replace(nlc, ' ')}...")

    model = bundle["model"]
    dim = model.get_sentence_embedding_dimension()
    sn_index, sn_cat = bundle["snomed"]["index"], bundle["snomed"]["catalog"]
    rx_index, rx_cat = bundle["rxnorm"]["index"], bundle["rxnorm"]["catalog"]
    sty_vocab, sty_emb = bundle["sty"]["vocab"], bundle["sty"]["emb"]

    # ---- 1) LLM expansion (multiple candidates allowed) ----
    if use_llm_clean and llm is not None:
        exp_cands = await llm_expand_query_async(llm, query, entity_type, bundle["sty"]["vocab"])
    else:
        exp_cands = [{"alternate_keywords": [], "description": "", "possible_stys": []}]

    # If somehow empty:
    if not exp_cands:
        exp_cands = [{"alternate_keywords": [], "description": "", "possible_stys": []}]

    expanded_summary = build_expanded_summary(exp_cands)

    # ---- 2) Prepare embeddings for signals ----
    # print(f"[LLM {datetime.now().strftime(DATE_FORMAT_STR)}] Preparing embeddings for: {query[:60].replace(nlc, ' ')}...")
    # ---- 2) Prepare embeddings for signals ----
    q_vec = (await embed_texts_async(model, [query], batch_size=4))[0:1]

    # helper: async FAISS search
    async def search_one_system_async(system: str, vec: np.ndarray, topk: int) -> List[Dict]:
        idx = sn_index if system == SNOMED else rx_index
        cat = sn_cat if system == SNOMED else rx_cat
        D, I = await faiss_search_async(idx, vec, topk)
        return build_row_hits(D[0], I[0], cat)

    async def batch_search_one_system_async(system: str, vecs: np.ndarray, topk: int) -> List[List[Dict]]:
        if vecs.shape[0] == 0:
            return []
        idx = sn_index if system == SNOMED else rx_index
        cat = sn_cat if system == SNOMED else rx_cat
        D, I = await faiss_search_async(idx, vecs, topk)
        return [build_row_hits(D[i], I[i], cat) for i in range(len(vecs))]

    def update_scores(system: str, row_list: List[Dict], contrib_name: str, score_getter):
        for r in row_list:
            key = (system, r["row_id"])
            if key not in row_scores:
                row_scores[key] = {**r, "score_components": {}, "composite": 0.0}
            row_scores[key]["score_components"][contrib_name] = score_getter(r)

    # Helper to run a vector search against one system
    def search_one_system(system: str, vec: np.ndarray, topk: int) -> List[Dict]:
        idx = sn_index if system == SNOMED else rx_index
        cat = sn_cat if system == SNOMED else rx_cat
        D,I = faiss_search(idx, vec, topk)
        return build_row_hits(D[0], I[0], cat)
    
    def batch_search_one_system(system: str, vecs: np.ndarray, topk: int) -> List[List[Dict]]:
        if vecs.shape[0] == 0:
            return []
        idx = sn_index if system == SNOMED else rx_index
        cat = sn_cat if system == SNOMED else rx_cat
        D, I = faiss_search(idx, vecs, topk)                      # vecs: (m, d)
        return [build_row_hits(D[i], I[i], cat) for i in range(len(vecs))]

    # For each candidate we will track a big dict of per-row scores and take max across candidates
    row_scores: Dict[Tuple[str,int], Dict[str, Any]] = {}  # key: (system,row_id)

    # For each candidate: description, alt keywords
    for cand in exp_cands:
        alt_kws = cand.get("alternate_keywords", [])
        desc = cand.get("description", "")
        pred_stys = cand.get("possible_stys", [])

        # ----- description -----
        # print(f"[LLM {datetime.now().strftime(DATE_FORMAT_STR)}] Candidate description matching: {desc[:60].replace(nlc, ' ')}...")
        if desc.strip():
            d_vec = (await embed_texts_async(model, [desc], batch_size=4))[0:1]
            for sys in (SNOMED, RXNORM):
                hits = await search_one_system_async(sys, d_vec, params.semantic_top_desc)
                update_scores(sys, hits, "desc", lambda r: cos_to_01(r["score"]))

        kw_vecs = await embed_texts_async(model, alt_kws, batch_size=min(64, len(alt_kws))) \
                if alt_kws else np.zeros((0, q_vec.shape[1]), np.float32)

        for sys in (SNOMED, RXNORM):
            batched_hits = await batch_search_one_system_async(sys, kw_vecs, params.semantic_top_kw_each)
            for hits in batched_hits:
                update_scores(sys, hits, "kw", lambda r: cos_to_01(r["score"]))

        # direct query (do this once per candidate group, as you have)
        for sys in (SNOMED, RXNORM):
            hits = await search_one_system_async(sys, q_vec, params.semantic_top_direct)
            update_scores(sys, hits, "direct", lambda r: cos_to_01(r["score"]))

        # ----- STY similarity -----
        # we don't search by STY; we just compute a STY match score for every row encountered so far
        # print(f"[LLM {datetime.now().strftime(DATE_FORMAT_STR)}] STY similarity for predicted STYs: {', '.join(pred_stys)[:60].replace(nlc, ' ')}...")
        if pred_stys:
            sty_map = precompute_sty_scores_map(pred_stys, sty_vocab, sty_emb)
            for store in row_scores.values():
                store["score_components"]["sty"] = sty_map.get(store.get("STY",""), 0.0)
        # print(f"[LLM {datetime.now().strftime(DATE_FORMAT_STR)}] Candidate matching done, found {len(row_scores)} unique rows so far.  ")

    # Fill missing components with zeros
    for store in row_scores.values():
        sc = store["score_components"]
        sc.setdefault("desc", 0.0)
        sc.setdefault("kw", 0.0)
        sc.setdefault("direct", 0.0)
        sc.setdefault("sty", 0.0)
        store["composite"] = (weights.w_desc * sc["desc"] +
                              weights.w_kw   * sc["kw"]   +
                              weights.w_direct * sc["direct"] +
                              weights.w_sty  * sc["sty"])

    # ---- 3) Take top pool and fuzzy re-rank ----
    print(f"[LLM {datetime.now().strftime(DATE_FORMAT_STR)}] Pooling top {params.fuzzy_pool} rows for fuzzy matching  for: {query[:60].replace(nlc, ' ')}...")
    # ---- 3) Take top pool and fuzzy re-rank ----
    pooled_rows = sorted(row_scores.values(), key=lambda x: x["composite"], reverse=True)[:params.fuzzy_pool]

    if HAVE_RAPIDFUZZ and pooled_rows:
        all_kws = []
        for c in exp_cands:
            all_kws.extend(c.get("alternate_keywords", [])[:30])
        choices = [r["STR"] for r in pooled_rows]
        fuzzy_arr = await fuzzy_scores_max_async(
            query=query,
            alt_keywords=list(dict.fromkeys(all_kws))[:30],
            choices=choices,
            kw_limit=8,
            workers=int(getattr(params, "fuzzy_workers", -1)),
            prefilter=max(200, params.top_pool_after_fuzzy * 2),
            use_two_stage=True
        )
        for r, sc in zip(pooled_rows, fuzzy_arr):
            r["fuzzy"] = float(sc)
        pooled_rows.sort(key=lambda x: x["fuzzy"], reverse=True)

    top_rows = pooled_rows[:params.top_pool_after_fuzzy]

    # ---- 4) Aggregate to per-code final score ----
    # Build helper maps
    print(f"[LLM {datetime.now().strftime(DATE_FORMAT_STR)}] Aggregating top {len(top_rows)} rows to codes for: {query[:60].replace(nlc, ' ')}...")
    cat_by_sys = {SNOMED: sn_cat, RXNORM: rx_cat}
    idx_by_sys = {SNOMED: sn_index, RXNORM: rx_index}

    # Precompute query representations used to recompute composite for reconstructed rows
    # We reuse the best candidate (max signals) approach: just re-evaluate components that exist (desc/kw/direct/sty)
    have_desc = any(c.get("description","").strip() for c in exp_cands)
    desc_vec = (await embed_texts_async(model, [max(exp_cands, key=lambda c: len(c.get("description","")))["description"]], 4))[0:1] \
            if have_desc else None

    all_kws = []
    for c in exp_cands: all_kws += c.get("alternate_keywords", [])
    kw_vecs = await embed_texts_async(model, list(dict.fromkeys(all_kws)),
                                    batch_size=min(64, max(1,len(all_kws)))) if all_kws else None
    
    pred_stys_all = []
    for c in exp_cands: pred_stys_all += c.get("possible_stys", [])
    pred_stys_all = list(dict.fromkeys(pred_stys_all))

    sty_map_all = precompute_sty_scores_map(pred_stys_all, sty_vocab, sty_emb)

    vec_by_sys = {SNOMED: bundle["snomed"]["vecs"], RXNORM: bundle["rxnorm"]["vecs"]}

    def _cos01(arr: np.ndarray) -> np.ndarray:
        # vectorized cos_to_01 for arrays
        return np.clip(0.5 * (arr + 1.0), 0.0, 1.0)

    # Gather all candidate codes present in top_rows
    codes = {}
    for r in top_rows:
        code = (r["System"], r["CODE"])
        codes.setdefault(code, {"rows": [], "rows_top": []})
        codes[code]["rows_top"].append(r)

    # Batched composite per code using memmapped vectors
    for (system, code), bucket in codes.items():
        cat = cat_by_sys[system]
        vecs_mm = vec_by_sys[system]
        all_rows = cat.loc[cat["CODE"] == code].copy()
        bucket["rows"] = []

        if all_rows.empty:
            continue

        row_ids = all_rows["row_id"].to_numpy(dtype=np.int64)
        row_vecs = vecs_mm[row_ids]                         # (num_rows, d)

        # components (all cosine on normalized embeddings)
        sc_dir = (row_vecs @ q_vec.T).ravel() if q_vec is not None else 0.0
        sc_desc = (row_vecs @ desc_vec.T).ravel() if desc_vec is not None else 0.0
        sc_kw = (row_vecs @ kw_vecs.T).max(axis=1) if (kw_vecs is not None and kw_vecs.shape[0] > 0) else 0.0

        sc_dir01  = _cos01(sc_dir)
        sc_desc01 = _cos01(sc_desc) if isinstance(sc_desc, np.ndarray) else 0.0
        sc_kw01   = _cos01(sc_kw)   if isinstance(sc_kw,   np.ndarray) else 0.0

        sc_sty = np.fromiter(
            (sty_map_all.get(str(sty), 0.0) for sty in all_rows["STY"]),
            count=len(all_rows), dtype=np.float32
        )

        comp = (weights.w_desc * sc_desc01 +
                weights.w_kw   * sc_kw01   +
                weights.w_direct * sc_dir01 +
                weights.w_sty  * sc_sty).astype(np.float32)

        bucket["rows"] = [{
            "row_id": int(rid),
            "System": system,
            "CODE": code,
            "STR": str_str,
            "STY": str_sty,
            "TTY": str_tty,
            "composite": float(c)
        } for rid, str_str, str_sty, str_tty, c in zip(
            row_ids,
            all_rows["STR"].tolist(),
            all_rows["STY"].tolist(),
            all_rows["TTY"].tolist(),
            comp
        )]

    # Compute final per-code score
    ranked_codes = []
    for (system, code), bucket in codes.items():
        rows_all = bucket["rows"]
        rows_top = bucket["rows_top"]
        if not rows_top or not rows_all:
            continue
        avg_all = float(np.mean([r["composite"] for r in rows_all]))
        avg_500 = float(np.mean([r["composite"] for r in rows_top]))
        # percentage of instances in top 500
        denom = max(1, len(rows_all))
        pct = (len(rows_top) / denom) * 100.0
        pct_safe = max(1.0001, pct)
        boost = math.sqrt(math.log10(pct_safe))
        final_score = avg_all * avg_500 * boost
        ranked_codes.append({
            "System": system, "CODE": code,
            "avg_all": avg_all, "avg_500": avg_500,
            "pct_in_top500": pct, "final": final_score
        })

    ranked_codes.sort(key=lambda x: x["final"], reverse=True)
    topN_codes = ranked_codes[:params.final_llm_candidates]

    # Materialize one representative row (best STR) per code for reporting & LLM rerank
    def best_name_for_code(system: str, code: str) -> Dict:
        cat = cat_by_sys[system]
        subset = cat.loc[cat["CODE"] == code]
        # choose the PT over SY where possible
        subset = subset.copy()
        subset["tty_rank"] = subset["TTY"].map({"PT":0, "FN":1, "SCD":1, "SBD":1}).fillna(5)
        win = subset.sort_values(["tty_rank"]).iloc[0]
        return {"STR": win["STR"], "STY": win["STY"], "TTY": win["TTY"]}

    topN_rows = []
    for c in topN_codes:
        meta_row = best_name_for_code(c["System"], c["CODE"])
        topN_rows.append({**c, **meta_row})

    # Optional LLM final rerank (codes only)
    if use_llm_rerank and llm is not None and len(topN_rows) > 0:
        print(f"[LLM {datetime.now().strftime(DATE_FORMAT_STR)}] Final LLM reranking among {len(topN_rows)} candidates for: {query[:60].replace(nlc, ' ')}...")
        cand_blob = "\n".join([f"{r['System']} | {r['CODE']} | {r['STY']} | {r['TTY']} | {r['STR']}" for r in topN_rows])
        final_codes = await llm_rerank_codes_async(llm, query, entity_type, expanded_summary, cand_blob, params.final_output_k)
        # Map LLM output order back to code rows (keep those that exist in our topN)
        code_set = {(r["System"], r["CODE"]) for r in topN_rows}
        # LLM returned only code string; pick the (system,code) with best 'final' if multiple systems share code text
        # Build index by code -> best pair
        best_by_code: Dict[str, Tuple[str,str,float]] = {}
        for r in topN_rows:
            tup = best_by_code.get(r["CODE"])
            if (tup is None) or (r["final"] > tup[2]):
                best_by_code[r["CODE"]] = (r["System"], r["CODE"], r["final"])
        final_rows = []
        for code in final_codes:
            if code in best_by_code:
                sys, c, _ = best_by_code[code]
                # retrieve representative row again
                rep = best_name_for_code(sys, c)
                final_rows.append({"System": sys, "CODE": c, **rep})
        # If LLM returns fewer than requested, fill from topN by 'final'
        seen = {(r["System"], r["CODE"]) for r in final_rows}
        for r in topN_rows:
            if (r["System"], r["CODE"]) not in seen and len(final_rows) < params.final_output_k:
                final_rows.append({"System": r["System"], "CODE": r["CODE"], "STR": r["STR"], "STY": r["STY"], "TTY": r["TTY"]})
        reason = "LLM rerank (RxNorm preferred for meds; SNOMED otherwise)"
        return final_rows, reason

    # No LLM rerank: return topN by final score (truncate to topk)
    final_rows = []
    for r in topN_rows[:params.final_output_k]:
        final_rows.append({"System": r["System"], "CODE": r["CODE"], "STR": r["STR"], "STY": r["STY"], "TTY": r["TTY"]})
    reason = "Advanced scoring without LLM rerank"

    print(f"[LLM {datetime.now().strftime(DATE_FORMAT_STR)}] Matching process complete for: {query[:60].replace(nlc, ' ')}...")
    return final_rows, reason

# -------------------- Legacy (non-LLM) selection for single/batch --------------------
def build_hits_simple(query: str, D_row: np.ndarray, I_row: np.ndarray, catalog: pd.DataFrame,
                      rerank: bool, topk: int) -> List[Dict]:
    hits = build_row_hits(D_row, I_row, catalog)
    if rerank and HAVE_RAPIDFUZZ and len(hits) > 1:
        hits.sort(key=lambda r: fuzz.token_set_ratio(query, r["STR"]), reverse=True)
    return hits[:topk]

def choose_system_hits(preferred_system: str,
                       hits_snomed: List[Dict],
                       hits_rxnorm: List[Dict],
                       min_score: float,
                       alt_margin: float) -> Tuple[str, List[Dict], str]:
    top_sn = hits_snomed[0]["score"] if hits_snomed else -1.0
    top_rx = hits_rxnorm[0]["score"] if hits_rxnorm else -1.0
    if preferred_system == SNOMED:
        pref_hits, alt_hits = hits_snomed, hits_rxnorm
        pref_name, alt_name = SNOMED, RXNORM
        pref_top, alt_top = top_sn, top_rx
    else:
        pref_hits, alt_hits = hits_rxnorm, hits_snomed
        pref_name, alt_name = RXNORM, SNOMED
        pref_top, alt_top = top_rx, top_sn
    if pref_top < min_score:
        if alt_top >= min_score:
            return alt_name, alt_hits, "fallback to alt"
        else:
            return pref_name, [], f"no match ≥ {min_score:.2f}"
    if alt_top >= min_score:
        rel_gain = (alt_top - pref_top) / max(1e-9, abs(pref_top))
        if rel_gain >= alt_margin:
            return alt_name, alt_hits, f"switch (gain {rel_gain:.2%})"
    return pref_name, pref_hits, "prefer preferred"

# -------------------- QUERY --------------------
def run_query(args):
    maybe_hf_login(args.hf_token)
    configure_cpu_threading(args.blas_threads, args.faiss_threads)
    blas_ctx = (threadpool_limits(limits=args.blas_threads)
                if (threadpool_limits and args.blas_threads) else nullcontext())
    bundle = load_bundle(args.index_dir)

    if bundle["meta"]["index_type"] == "hnsw":
        for key in ("snomed", "rxnorm"):
            faiss.downcast_index(bundle[key]["index"].index).hnsw.efSearch = args.hnsw_efs

    with blas_ctx:
        llm = None
        if args.use_llm_clean or args.use_llm_rerank:
            llm = LLMClient(LLMConfig(backend=args.llm_backend,
                                    hf_model_id=args.llm_hf_model_id,
                                    gguf_path=args.llm_gguf_path,
                                    max_new_tokens=args.llm_max_new_tokens,
                                    temperature=args.llm_temperature,
                                    top_p=args.llm_top_p))

        if args.use_llm_clean or args.use_llm_rerank:
            # Advanced pipeline on both systems
            rows, reason = advanced_match_one(
                query=args.text,
                entity_type=args.entity_type,
                bundle=bundle,
                llm=llm,
                use_llm_clean=args.use_llm_clean,
                use_llm_rerank=args.use_llm_rerank,
                weights=AdvancedWeights(args.weights_desc, args.weights_kw, args.weights_direct, args.weights_sty),
                params=AdvancedParams(final_output_k=args.topk, fuzzy_workers=args.fuzzy_workers),
                rerank_mode=args.rerank
            )
            print(f"\n[Query][Advanced] chosen ({reason})")
            if not rows:
                print("No match found.")
                return
            best = rows[0]
            print(f"Top-1: CODE={best['CODE']} | DESC={best['STR']} | STY={best['STY']} | TTY={best['TTY']} | SYS={best['System']}")
            print("\nTop-k candidates:")
            for i, r in enumerate(rows, 1):
                print(f"{i:>2}. {r['System']} | {r['CODE']} | {r['STY']} | {r['TTY']} | {r['STR']}")
            return

    # Legacy path (no LLM): your earlier policy
    model = bundle["model"]
    qv = embed_texts(model, [args.text], batch_size=1)
    D_sn, I_sn = bundle["snomed"]["index"].search(qv, args.topk)
    D_rx, I_rx = bundle["rxnorm"]["index"].search(qv, args.topk)

    hits_sn = build_hits_simple(args.text, D_sn[0], I_sn[0], bundle["snomed"]["catalog"], rerank=(args.rerank=="rapidfuzz"), topk=args.topk)
    hits_rx = build_hits_simple(args.text, D_rx[0], I_rx[0], bundle["rxnorm"]["catalog"], rerank=(args.rerank=="rapidfuzz"), topk=args.topk)

    preferred = choose_system(args.entity_type)
    chosen_sys, chosen_hits, reason = choose_system_hits(preferred, hits_sn, hits_rx, args.min_score, args.alt_margin)

    print(f"\n[Query][Legacy] preferred={preferred} -> chosen={chosen_sys} ({reason})")
    if not chosen_hits:
        print(f"No match ≥ {args.min_score:.2f}")
        return
    best = chosen_hits[0]
    print(f"Top-1: CODE={best['CODE']} | DESC={best['STR']} | STY={best['STY']} | TTY={best['TTY']} | SYS={best['System']}")
    print("\nTop-k candidates:")
    for i, r in enumerate(chosen_hits, 1):
        print(f"{i:>2}. {r['System']} | {r['CODE']} | {r['STY']} | {r['TTY']} | {r['STR']}")

# -------------------- BATCH --------------------
def read_input_table(path: str) -> pd.DataFrame:
    ext = os.path.splitext(path.lower())[1]
    if ext in (".xlsx", ".xls"):
        df = pd.read_excel(path)
    elif ext in (".csv", ".tsv"):
        sep = "," if ext == ".csv" else "\t"
        df = pd.read_csv(path, sep=sep)
    else:
        raise ValueError("Unsupported input format. Use .xlsx/.xls or .csv/.tsv")
    cols_norm = {c: c.strip() for c in df.columns}
    df = df.rename(columns=cols_norm)
    if REQ_INPUT_COL not in df.columns or REQ_TYPE_COL not in df.columns:
        raise ValueError(f"Input must have columns: '{REQ_INPUT_COL}' and '{REQ_TYPE_COL}'")
    return df

def write_output_table(df: pd.DataFrame, path: str):
    ext = os.path.splitext(path.lower())[1]
    if ext in (".xlsx", ".xls"):
        df.to_excel(path, index=False)
    elif ext in (".csv", ".tsv"):
        sep = "," if ext == ".csv" else "\t"
        df.to_csv(path, index=False, sep=sep)
    else:
        raise ValueError("Unsupported output format. Use .xlsx/.xls or .csv/.tsv")

# def run_batch(args):
#     maybe_hf_login(args.hf_token)
#     bundle = load_bundle(args.index_dir)

#     if bundle["meta"]["index_type"] == "hnsw":
#         for key in ("snomed", "rxnorm"):
#             faiss.downcast_index(bundle[key]["index"].index).hnsw.efSearch = args.hnsw_efs

#     df = read_input_table(args.in_file)

#     llm = None
#     if args.use_llm_clean or args.use_llm_rerank:
#         llm = LLMClient(LLMConfig(backend=args.llm_backend,
#                                   hf_model_id=args.llm_hf_model_id,
#                                   gguf_path=args.llm_gguf_path,
#                                   max_new_tokens=args.llm_max_new_tokens,
#                                   temperature=args.llm_temperature,
#                                   top_p=args.llm_top_p))

#     # Prepare columns
#     for col in (OUT_SYS_COL, OUT_CODE_COL, OUT_DESC_COL, OUT_STY_COL, OUT_TTY_COL):
#         if col not in df.columns:
#             df[col] = ""

#     if args.include_topk:
#         for k in range(1, args.topk + 1):
#             for base, col in [
#                 ("System",    f"Output Coding System {k}"),
#                 ("CODE",      f"Output Target Code {k}"),
#                 ("STR",       f"Output Target Description {k}"),
#                 ("STY",       f"Output Semantic Type {k}"),
#                 ("TTY",       f"Output Term Type {k}"),
#             ]:
#                 if col not in df.columns:
#                     df[col] = ""

#     # Process each row
#     for i in tqdm(range(len(df)), desc="[Batch] LLM+ANN"):
#         text = str(df.at[i, REQ_INPUT_COL])
#         etype = str(df.at[i, REQ_TYPE_COL])

#         if args.use_llm_clean or args.use_llm_rerank:
#             rows, _reason = advanced_match_one(
#                 query=text,
#                 entity_type=etype,
#                 bundle=bundle,
#                 llm=llm,
#                 use_llm_clean=args.use_llm_clean,
#                 use_llm_rerank=args.use_llm_rerank,
#                 weights=AdvancedWeights(args.weights_desc, args.weights_kw, args.weights_direct, args.weights_sty),
#                 params=AdvancedParams(final_output_k=args.topk),
#                 rerank_mode=args.rerank
#             )
#             if rows:
#                 top1 = rows[0]
#                 df.at[i, OUT_SYS_COL]  = top1["System"]
#                 df.at[i, OUT_CODE_COL] = top1["CODE"]
#                 df.at[i, OUT_DESC_COL] = top1["STR"]
#                 df.at[i, OUT_STY_COL]  = top1["STY"]
#                 df.at[i, OUT_TTY_COL]  = top1["TTY"]
#                 if args.include_topk:
#                     for k, r in enumerate(rows, 1):
#                         if k > args.topk: break
#                         df.at[i, f"Output Coding System {k}"]      = r["System"]
#                         df.at[i, f"Output Target Code {k}"]        = r["CODE"]
#                         df.at[i, f"Output Target Description {k}"] = r["STR"]
#                         df.at[i, f"Output Semantic Type {k}"]      = r["STY"]
#                         df.at[i, f"Output Term Type {k}"]          = r["TTY"]
#             continue

#         # Legacy path (no LLM)
#         model = bundle["model"]
#         qv = embed_texts(model, [text], batch_size=1)
#         D_sn, I_sn = bundle["snomed"]["index"].search(qv, args.topk)
#         D_rx, I_rx = bundle["rxnorm"]["index"].search(qv, args.topk)
#         hits_sn = build_hits_simple(text, D_sn[0], I_sn[0], bundle["snomed"]["catalog"], rerank=(args.rerank=="rapidfuzz"), topk=args.topk)
#         hits_rx = build_hits_simple(text, D_rx[0], I_rx[0], bundle["rxnorm"]["catalog"], rerank=(args.rerank=="rapidfuzz"), topk=args.topk)
#         preferred = choose_system(etype)
#         chosen_sys, chosen_hits, _reason = choose_system_hits(preferred, hits_sn, hits_rx, args.min_score, args.alt_margin)
#         if chosen_hits:
#             top1 = chosen_hits[0]
#             df.at[i, OUT_SYS_COL]  = top1["System"]
#             df.at[i, OUT_CODE_COL] = top1["CODE"]
#             df.at[i, OUT_DESC_COL] = top1["STR"]
#             df.at[i, OUT_STY_COL]  = top1["STY"]
#             df.at[i, OUT_TTY_COL]  = top1["TTY"]
#             if args.include_topk:
#                 for k, r in enumerate(chosen_hits, 1):
#                     df.at[i, f"Output Coding System {k}"]      = r["System"]
#                     df.at[i, f"Output Target Code {k}"]        = r["CODE"]
#                     df.at[i, f"Output Target Description {k}"] = r["STR"]
#                     df.at[i, f"Output Semantic Type {k}"]      = r["STY"]
#                     df.at[i, f"Output Term Type {k}"]          = r["TTY"]

#     write_output_table(df, args.out_file)
#     print(f"[Batch] Wrote predictions to: {args.out_file}")

def run_batch(args):
    maybe_hf_login(args.hf_token)
    bundle = load_bundle(args.index_dir)

    if bundle["meta"]["index_type"] == "hnsw":
        for key in ("snomed", "rxnorm"):
            faiss.downcast_index(bundle[key]["index"].index).hnsw.efSearch = args.hnsw_efs
    
    configure_cpu_threading(args.blas_threads, args.faiss_threads)
    blas_ctx = (threadpool_limits(limits=args.blas_threads)
                if (threadpool_limits and args.blas_threads) else nullcontext())

    df = read_input_table(args.in_file)

    # Prepare output columns (same as your version) ...
    for col in (OUT_SYS_COL, OUT_CODE_COL, OUT_DESC_COL, OUT_STY_COL, OUT_TTY_COL):
        if col not in df.columns:
            df[col] = ""
    if args.include_topk:
        for k in range(1, args.topk + 1):
            for name in (f"Output Coding System {k}",
                         f"Output Target Code {k}",
                         f"Output Target Description {k}",
                         f"Output Semantic Type {k}",
                         f"Output Term Type {k}"):
                if name not in df.columns:
                    df[name] = ""

    # Build LLM (async)
    llm = None
    if args.use_llm_clean or args.use_llm_rerank:
        backend, hf_id, gguf = pick_llm_backend_and_model(args.llm_backend,
                                                          args.llm_hf_model_id,
                                                          args.llm_gguf_path)
        cfg = LLMConfig(
            backend=backend,
            hf_model_id=hf_id,
            gguf_path=gguf,
            max_new_tokens=args.llm_max_new_tokens,
            temperature=args.llm_temperature,
            top_p=args.llm_top_p,
            vllm_quantization=args.vllm_quantization,
            tp=args.llm_tp,
            n_threads=args.llm_n_threads,
            concurrency=args.llm_concurrency or default_concurrency(),
        )
        llm = AsyncLLMClient(cfg)

    async def process_row(i: int):
        text = str(df.at[i, REQ_INPUT_COL])
        etype = str(df.at[i, REQ_TYPE_COL])
        if args.use_llm_clean or args.use_llm_rerank:
            rows, _ = await advanced_match_one_async(
                query=text,
                entity_type=etype,
                bundle=bundle,
                llm=llm,
                use_llm_clean=args.use_llm_clean,
                use_llm_rerank=args.use_llm_rerank,
                weights=AdvancedWeights(args.weights_desc, args.weights_kw, args.weights_direct, args.weights_sty),
                params=AdvancedParams(final_output_k=args.topk, fuzzy_workers=args.fuzzy_workers),
                rerank_mode=args.rerank
            )
            if rows:
                top1 = rows[0]
                df.at[i, OUT_SYS_COL]  = top1["System"]
                df.at[i, OUT_CODE_COL] = top1["CODE"]
                df.at[i, OUT_DESC_COL] = top1["STR"]
                df.at[i, OUT_STY_COL]  = top1["STY"]
                df.at[i, OUT_TTY_COL]  = top1["TTY"]
                if args.include_topk:
                    for k, r in enumerate(rows, 1):
                        if k > args.topk: break
                        df.at[i, f"Output Coding System {k}"]      = r["System"]
                        df.at[i, f"Output Target Code {k}"]        = r["CODE"]
                        df.at[i, f"Output Target Description {k}"] = r["STR"]
                        df.at[i, f"Output Semantic Type {k}"]      = r["STY"]
                        df.at[i, f"Output Term Type {k}"]          = r["TTY"]
            return

        # legacy (no LLM) — keep your synchronous block
        model = bundle["model"]
        qv = embed_texts(model, [text], batch_size=1)
        D_sn, I_sn = bundle["snomed"]["index"].search(qv, args.topk)
        D_rx, I_rx = bundle["rxnorm"]["index"].search(qv, args.topk)
        hits_sn = build_hits_simple(text, D_sn[0], I_sn[0], bundle["snomed"]["catalog"], rerank=(args.rerank=="rapidfuzz"), topk=args.topk)
        hits_rx = build_hits_simple(text, D_rx[0], I_rx[0], bundle["rxnorm"]["catalog"], rerank=(args.rerank=="rapidfuzz"), topk=args.topk)
        preferred = choose_system(etype)
        chosen_sys, chosen_hits, _ = choose_system_hits(preferred, hits_sn, hits_rx, args.min_score, args.alt_margin)
        if chosen_hits:
            top1 = chosen_hits[0]
            df.at[i, OUT_SYS_COL]  = top1["System"]
            df.at[i, OUT_CODE_COL] = top1["CODE"]
            df.at[i, OUT_DESC_COL] = top1["STR"]
            df.at[i, OUT_STY_COL]  = top1["STY"]
            df.at[i, OUT_TTY_COL]  = top1["TTY"]
            if args.include_topk:
                for k, r in enumerate(chosen_hits, 1):
                    df.at[i, f"Output Coding System {k}"]      = r["System"]
                    df.at[i, f"Output Target Code {k}"]        = r["CODE"]
                    df.at[i, f"Output Target Description {k}"] = r["STR"]
                    df.at[i, f"Output Semantic Type {k}"]      = r["STY"]
                    df.at[i, f"Output Term Type {k}"]          = r["TTY"]

    async def runner():
        loop = asyncio.get_running_loop()
        cpu_pool_size = int(args.cpu_pool or max(32, multiprocessing.cpu_count()))
        pool = concurrent.futures.ThreadPoolExecutor(max_workers=cpu_pool_size)
        loop.set_default_executor(pool)
        os.environ["CPU_POOL"] = str(cpu_pool_size)  # used by _init_sems()

        rows_conc = int(getattr(args, "rows_concurrency", None)
                or getattr(args, "llm_concurrency", None)
                or (llm.cfg.concurrency if llm else default_concurrency()))

        q = asyncio.Queue()
        for i in range(len(df)):
            await q.put(i)
        for _ in range(rows_conc):
            await q.put(None)  # sentinels

        pbar = tqdm(total=len(df), desc="[Batch] LLM+ANN (async)")

        async def worker():
            while True:
                i = await q.get()
                print("[Worker] Started worker with index:", i)
                if i is None:
                    break
                try:
                    await process_row(i)
                finally:
                    pbar.update(1)

        workers = [asyncio.create_task(worker()) for _ in range(rows_conc)]
        await asyncio.gather(*workers)
        pbar.close()

        pool.shutdown(wait=True)

    with blas_ctx:
        if args.use_llm_clean or args.use_llm_rerank:
            asyncio.run(runner())
        else:
            # legacy loop
            for i in tqdm(range(len(df)), desc="[Batch] Legacy ANN"):
                asyncio.run(process_row(i))  # safe since process_row handles legacy

    write_output_table(df, args.out_file)
    print(f"[Batch] Wrote predictions to: {args.out_file}")

# -------------------- CLI --------------------
def main():
    p = argparse.ArgumentParser(description="Clinical Harmonization Matcher (FAISS + LLM Assist)")

    sub = p.add_subparsers(dest="cmd", required=True)

    # BUILD
    pb = sub.add_parser("build", help="Build FAISS indices & STY embeddings.")
    pb.add_argument("--snomed_parquet", required=True)
    pb.add_argument("--rxnorm_parquet", required=True)
    pb.add_argument("--out_dir", required=True)
    pb.add_argument("--model", default="google/embeddinggemma-300m")
    pb.add_argument("--index", choices=["hnsw", "ivfpq", "flat"], default="hnsw")
    pb.add_argument("--batch", type=int, default=1024)
    pb.add_argument("--hnsw_m", type=int, default=32)
    pb.add_argument("--hnsw_efc", type=int, default=200)
    pb.add_argument("--hnsw_efs", type=int, default=128)
    pb.add_argument("--ivf_nlist", type=int, default=16384)
    pb.add_argument("--pq_m", type=int, default=64)
    pb.add_argument("--pq_nbits", type=int, default=8)
    pb.add_argument("--hf_token", default=None)

    # QUERY
    pq = sub.add_parser("query", help="Single query lookup.")
    pq.add_argument("--index_dir", required=True)
    pq.add_argument("--text", required=True)
    pq.add_argument("--entity_type", required=True)
    pq.add_argument("--topk", type=int, default=10)
    pq.add_argument("--hnsw_efs", type=int, default=128)
    pq.add_argument("--rerank", choices=["none", "rapidfuzz"], default="rapidfuzz")
    pq.add_argument("--min_score", type=float, default=0.30)
    pq.add_argument("--alt_margin", type=float, default=0.15)
    # LLM options
    pq.add_argument("--use_llm_clean", type=lambda s: s.lower()!="false", default=True)
    pq.add_argument("--use_llm_rerank", type=lambda s: s.lower()!="false", default=True)
    pq.add_argument("--llm_backend", choices=["auto","vllm","llama-cpp"], default="auto")
    pq.add_argument("--llm_hf_model_id", default=UNSLOTH_VLLM_OSS_20B)
    pq.add_argument("--llm_gguf_path", default=None)
    pq.add_argument("--llm_max_new_tokens", type=int, default=512)
    pq.add_argument("--llm_temperature", type=float, default=0.1)
    pq.add_argument("--llm_top_p", type=float, default=0.9)
    pq.add_argument("--weights_desc", type=float, default=0.30)
    pq.add_argument("--weights_kw", type=float, default=0.40)
    pq.add_argument("--weights_direct", type=float, default=0.20)
    pq.add_argument("--weights_sty", type=float, default=0.10)
    pq.add_argument("--hf_token", default=None)

    # BATCH
    pba = sub.add_parser("batch", help="Batch process queries from CSV/XLSX.")
    pba.add_argument("--index_dir", required=True)
    pba.add_argument("--in_file", required=True)
    pba.add_argument("--out_file", required=True)
    pba.add_argument("--topk", type=int, default=10)
    pba.add_argument("--include_topk", action="store_true")
    pba.add_argument("--hnsw_efs", type=int, default=128)
    pba.add_argument("--rerank", choices=["none", "rapidfuzz"], default="rapidfuzz")
    pba.add_argument("--min_score", type=float, default=0.30)
    pba.add_argument("--alt_margin", type=float, default=0.15)
    # LLM options
    pba.add_argument("--use_llm_clean", type=lambda s: s.lower()!="false", default=True)
    pba.add_argument("--use_llm_rerank", type=lambda s: s.lower()!="false", default=True)
    pba.add_argument("--llm_backend", choices=["auto","vllm","llama-cpp"], default="auto")
    pba.add_argument("--llm_hf_model_id", default=UNSLOTH_VLLM_OSS_20B)
    pba.add_argument("--llm_gguf_path", default=None)
    pba.add_argument("--llm_max_new_tokens", type=int, default=512)
    pba.add_argument("--llm_temperature", type=float, default=0.1)
    pba.add_argument("--llm_top_p", type=float, default=0.9)
    pba.add_argument("--weights_desc", type=float, default=0.30)
    pba.add_argument("--weights_kw", type=float, default=0.40)
    pba.add_argument("--weights_direct", type=float, default=0.20)
    pba.add_argument("--weights_sty", type=float, default=0.10)
    pba.add_argument("--hf_token", default=None)

    # common LLM scaling knobs
    pq.add_argument("--llm_concurrency", type=int, default=None,
                    help="Max concurrent LLM requests. Default: 32*num_gpus or 2*(cores-1).")
    pq.add_argument("--llm_tp", default="auto",
                    help="Tensor parallel size for vLLM (int or 'auto').")
    pq.add_argument("--llm_n_threads", default="auto",
                    help="Threads per llama-cpp context (int or 'auto').")
    pq.add_argument("--vllm_quantization", default=DEFAULT_VLLM_QUANT,
                    help="Set vLLM quantization (e.g., mxfp4, awq, gptq).")

    pba.add_argument("--llm_concurrency", type=int, default=None)
    pba.add_argument("--llm_tp", default="auto")
    pba.add_argument("--llm_n_threads", default="auto")
    pba.add_argument("--vllm_quantization", default=DEFAULT_VLLM_QUANT)
    pba.add_argument("--rows_concurrency", type=int, default=20,
                 help="Max concurrent rows (pipeline-level). Defaults to llm_concurrency.")

    # >>> UPDATE START
    # CPU / threading knobs
    pq.add_argument("--blas_threads", type=int, default=None,
                    help="Threads for NumPy/BLAS (OpenBLAS/MKL/BLIS).")
    pq.add_argument("--faiss_threads", type=int, default=None,
                    help="OpenMP threads for FAISS.")
    pq.add_argument("--cpu_pool", type=int, default=None,
                    help="Default asyncio thread-pool size for CPU offloaded work (to_thread).")
    pq.add_argument("--fuzzy_workers", type=int, default=-1,
                    help="Workers for RapidFuzz (cdist). -1 = all cores.")

    pba.add_argument("--blas_threads", type=int, default=None)
    pba.add_argument("--faiss_threads", type=int, default=None)
    pba.add_argument("--cpu_pool", type=int, default=None)
    pba.add_argument("--fuzzy_workers", type=int, default=-1)
    # <<< UPDATE END

    args = p.parse_args()

    if args.cmd == "build":
        build_indices(args)
    elif args.cmd == "query":
        run_query(args)
    elif args.cmd == "batch":
        run_batch(args)

if __name__ == "__main__":
    main()

readme.md:
# Clinical Concept Harmonizer (Semantic + Fuzzy + LLM-Assist)

> HiLabs Hackathon 2025 — end-to-end engine to normalize messy clinical inputs into **RxNorm** (medications) and **SNOMED CT** (diagnoses/procedures/labs) with a hybrid of embedding search, fast fuzzy matching, and optional LLM assist. 

---

## Table of contents

* [What this does](#what-this-does)
* [How it works (architecture)](#how-it-works-architecture)
* [Why it’s novel](#why-its-novel)
* [Repo layout](#repo-layout)
* [Quickstart](#quickstart)
* [Build the indices](#build-the-indices)
* [Run: batch or single query](#run-batch-or-single-query)
* [Output format](#output-format)
* [Tuning & performance knobs](#tuning--performance-knobs)
* [Methodology details](#methodology-details)
* [Rules compliance](#rules-compliance)
* [Deliverables checklist (for evaluators)](#deliverables-checklist-for-evaluators)
* [Troubleshooting](#troubleshooting)
* [Notes on data & licensing](#notes-on-data--licensing)

---

## What this does

Given raw, messy clinical inputs (e.g., “paracetamol 500”, “chest xr”, “hcv rna”), the system:

1. Chooses the appropriate target coding system: **RxNorm** for medications; **SNOMED CT** elsewhere.
2. Uses **semantic search** (SentenceTransformers + FAISS) over pre-built indices.
3. Adds **fuzzy matching** (RapidFuzz) for robustness to spelling/abbreviation noise.
4. Optionally uses an **LLM in two places**:

   * **Query expansion**: propose alternate keywords, short description and likely semantic types (STYs).
   * **Final re-ranking**: reorder top candidates with a short justification.
5. Returns **top-k** normalized codes with system, code, description, STY, and TTY — written back to Excel/CSV.

You can run **LLM-off** (fast, fully open-source) or **LLM-on** (more recall on ambiguous inputs). Works on **CPU** via `llama-cpp-python` and **GPU** via `vLLM` (Qwen3-4B by default; auto-selects based on VRAM).

---

## How it works (architecture)

```
Raw input row ─► [Optional LLM: query expansion]
                    • alternate keywords
                    • short description
                    • candidate STYs (from known STY inventory)
                 ┌──────────────────────────────────────────────────────────┐
                 │ Semantic search (FAISS)                                  │
                 │  • direct query vec                                      │
                 │  • description vec                                       │
                 │  • each alternate keyword vec                            │
                 │  • compute STY match via pre-embedded STY vocabulary     │
                 └──────────────────────────────────────────────────────────┘
                                   │
                     Weighted multi-signal score per row
                     (desc/keywords/direct/STY)  →  pool top N rows
                                   │
                 Fast fuzzy re-rank (RapidFuzz, batched two-stage)
                                   │
                       Aggregate to per-code scores
             avg(all rows) × avg(top rows) × √log10(%rows in top pool)
                                   │
          [Optional LLM: final XML re-rank of top codes with reasons]
                                   │
                          Top-k codes to spreadsheet
```

**Key artifacts built once**

* SentenceTransformer embeddings for all SNOMED/RxNorm terms.
* FAISS indices (`HNSW` by default; `IVFPQ` and `Flat` also supported).
* A compact **STY vocabulary** and its embedding matrix for fast STY similarity.

---

## Why it’s novel

* **Multi-signal scoring**: combines four orthogonal signals

  * `direct` (input ↔ code term), `desc` (LLM description ↔ term),
  * `kw` (LLM keywords ↔ term), `sty` (predicted STY ↔ code STY).
* **Large-K pipeline**: searches large candidate pools efficiently, then **fuzzy re-ranks** with a **two-stage batched** RapidFuzz (`ratio` prefilter → `token_set_ratio` refine).
* **Per-code aggregation**: scores *families* of rows (same code, different TTY/strings) with a stability boost based on how many instances survive into the top pool.
* **LLM exchange format is strict XML** for deterministic parsing; LLM is *optional* and fully replaceable with the legacy pipeline.
* **Scalable & resource-aware**: caps FAISS/BLAS threads, sizes the asyncio CPU pool, and bounds concurrent GPU/CPU work with semaphores to avoid oversubscription.

---

## Repo layout

```
.
├─ clean.py                     # main pipeline (build/query/batch + LLM/FAISS/fuzzy)
├─ solution-notebook-demo.ipynb # notebook walkthrough of the solution, simple demo
├─ snomed_all_data.parquet      # SNOMED CT data (provided)
├─ rxnorm_all_data.parquet      # RxNorm data (provided)
├─ Test.xlsx                    # input sheet (sample cases)
├─ Test_with_predictions.xlsx   # (output file)
├─ Column Reference Guide.md    # column dictionary for the parquet files
├─ Arch.md                      # contains extremely detailed description about the architecture of our solution
└─ README.md                    # this file
```

> The screenshot near the top shows an example directory view.

---

## Quickstart

> **Python 3.10+** recommended.

### 1) Install dependencies

**CPU-only (simplest)**

```bash
pip install -U pandas pyarrow openpyxl numpy tqdm \
  sentence-transformers faiss-cpu rapidfuzz llama-cpp-python
```

**GPU (optional, for faster LLM / embeddings)**

```bash
pip install -U vllm bitsandbytes "vllm[flashinfer]" torch --index-url https://download.pytorch.org/whl/cu121
# Ensure a matching CUDA toolkit/driver is present on your machine.
```

> If you’re on a Debian/Ubuntu runtime that needs CUDA libs, see the commands in the next section. For local machines with proper GPU drivers, you usually **don’t** need `apt` steps.

---

## Build the indices

This step embeds all SNOMED/RxNorm strings and creates FAISS indices + STY embeddings.

```bash
python clean.py build \
  --snomed_parquet snomed_all_data.parquet \
  --rxnorm_parquet rxnorm_all_data.parquet \
  --out_dir indices
```

**Optional Hugging Face login** (if a model requires it):

```bash
python clean.py build ... --hf_token YOUR_HF_TOKEN
```

**Expected artifacts in `indices/`**

* `snomed.index.faiss`, `rxnorm.index.faiss`
* `snomed_catalog.parquet`, `rxnorm_catalog.parquet`
* `snomed_vectors.f32`, `rxnorm_vectors.f32` (memmaps)
* `sty_vocab.json`, `sty_embeddings.npy`
* `meta.json` (all settings captured)

**Build time (rough order of magnitude)**

* ~10–60 minutes depending on hardware and index type/size. HNSW is the default and works well.

---

## Run: batch or single query

### A) **Batch** (recommended)

**LLM OFF (pure semantic + fuzzy)** — fastest

```bash
python clean.py batch \
  --index_dir indices \
  --in_file Test.xlsx \
  --out_file out.xlsx \
  --use_llm_clean false --use_llm_rerank false \
  --llm_backend auto \
  --llm_concurrency 200 --rows_concurrency 128
```

**LLM ON (query expansion + final re-rank)** — best for ambiguity

```bash
python clean.py batch \
  --index_dir indices \
  --in_file Test.xlsx \
  --out_file out.xlsx \
  --use_llm_clean true --use_llm_rerank true \
  --llm_backend auto \
  --llm_concurrency 200 --rows_concurrency 128
```

Want the full top-k in your spreadsheet?

```bash
python clean.py batch \
  --index_dir indices \
  --in_file Test.xlsx --out_file out.xlsx \
  --use_llm_clean false --use_llm_rerank false \
  --include_topk true \
  --llm_concurrency 200 --rows_concurrency 128
```

### B) **Single query** (stdout)

```bash
python clean.py query \
  --text "hcv rna" \
  --entity_type "Procedure" \
  --index_dir indices \
  --use_llm_clean false --use_llm_rerank false \
  --llm_backend auto
```

---

## Output format

For each input row (`Input Entity Description`, `Entity Type`) you get:

* **Output Coding System** → `SNOMEDCT_US` or `RXNORM`
* **Output Target Code** → code as **string**
* **Output Target Description** → human-readable label
* **Output Semantic Type (STY)** → e.g., *Disease or Syndrome*, *Pharmacologic Substance*
* **Output Term Type (TTY)** → e.g., `PT`, `SCD`, `BN`, …

With `--include_topk`, additional columns are appended for each rank `k`:
`Output Coding System k`, `Output Target Code k`, `Output Target Description k`, `Output Semantic Type k`, `Output Term Type k`.

See **Column Reference Guide.md** for vocabulary column definitions used inside the engine.

---

## Tuning & performance knobs

**Index**

* `--index {hnsw,ivfpq,flat}` | `--hnsw_m` `--hnsw_efc` `--hnsw_efs` | `--ivf_nlist` `--pq_m` `--pq_nbits`

**LLM (optional)**

* `--llm_backend {auto,vllm,llama-cpp}`
* `--llm_hf_model_id`, `--llm_gguf_path` (auto-selected to Qwen3-4B GGUF on CPU)
* `--llm_concurrency`, `--llm_tp` (tensor parallel for vLLM), `--llm_n_threads` (llama-cpp)
* `--vllm_quantization` (defaults to bitsandbytes)

**Batch concurrency & CPU threads**

* `--rows_concurrency` (how many rows processed concurrently; defaults to LLM concurrency)
* `--cpu_pool` (sized ThreadPool for `asyncio.to_thread` work)
* `--faiss_threads` (FAISS OpenMP threads)
* `--blas_threads` (NumPy/MKL/OpenBLAS cap via threadpoolctl)

**Fuzzy**

* `--fuzzy_workers` (RapidFuzz cdist; `-1` = all cores)

**Scoring weights**

* `--weights_desc` `--weights_kw` `--weights_direct` `--weights_sty` (default `0.30/0.40/0.20/0.10`)

**Legacy rerank switch**

* `--rerank {none,rapidfuzz}` (when LLM is off)

---

## Methodology details

### Embeddings & FAISS

* SentenceTransformer encodes every `STR` in SNOMED/RxNorm (L2-normalized float32).
* FAISS index type is configurable (`HNSW` default).
* Search returns similarities that we map to `[0,1]` by `cos_to_01(x) = clip((x+1)/2)`.

### LLM-assisted **query expansion**

* System prompt constrains responses to **strict XML** with one or more `<candidate>` blocks:

  * `<alternate_keywords>` (brand/colloquial terms included)
  * `<description>` (1–3 sentences)
  * `<possible_semantic_term_types>` (from our STY inventory)
* We **cap to ~5 candidates** and feed these signals into the semantic search.

### Multi-signal scoring (per-row)

For each candidate row we compute:

* **desc**: cosine(description, STR)
* **kw**: max cosine over all alternate keyword vectors
* **direct**: cosine(input text, STR)
* **sty**: STY similarity via a **pre-embedded STY vocabulary** (`sty_vocab`, `sty_embeddings.npy`)

Weighted sum:

```
composite = wd*desc + wk*kw + wr*direct + ws*sty
```

### Fuzzy re-rank (stability to noise)

* Build a **pool** of the best rows (e.g., 500 by composite), then re-rank by **RapidFuzz**:

  * Stage-1: fast `ratio` to prefilter choices for each anchor (query + top alt keywords)
  * Stage-2: `token_set_ratio` on the reduced set
* Implemented as **batched cdist** calls with parallel workers.

### Per-code aggregation

Many vocabularies have multiple rows per code (synonyms, forms, etc.). We:

1. Reconstruct or load vectors for **all rows** of each surviving code.
2. Recompute the components against the best available representations (query/desc/kws).
3. Score the **code** by:

```
final = mean(composite over all rows) *
        mean(composite over top-pool rows for that code) *
        sqrt(log10( percentage_of_code_rows_in_top_pool ))
```

This rewards codes that are **consistently** strong across their variants.

### Optional LLM final re-rank

* We send a compact list: `system | code | STY | TTY | STR` for the top candidates.
* The LLM outputs strict XML `<choice>` blocks with `<code>` + `<reasoning>`.
* Output is intersected with our candidates (we keep order) and filled if needed.

### Resource-awareness

* `configure_cpu_threading()` sets FAISS OMP threads and caps BLAS pools; heavy sections run inside a `threadpool_limits` context.
* Async path sizes a single `ThreadPoolExecutor` and sets `CPU_POOL` env; internal semaphores for embeddings/FAISS use that to avoid oversubscription.

---

## Rules compliance

* **Open-source only**: uses SentenceTransformers, FAISS, RapidFuzz, vLLM/llama-cpp, PyTorch — all open-source.
* **No external/proprietary APIs**: LLMs run locally via vLLM or llama-cpp; Hugging Face downloads public models if you opt in.
* **Outputs include**: system, code, description (+ STY/TTY) exactly as requested.

---

## Deliverables checklist (for evaluators)

* ✅ **Public repo** with runnable code (`clean.py`)
* ✅ **README** (this file) explaining approach, setup, and usage
* ✅ **Run script examples** under “Quickstart / Run”
* ✅ **Produces outputs** with required columns
* ✅ **Handles ambiguity/noise** via LLM expansion + fuzzy re-rank
* ✅ **Complete docs** for dataset columns (see *Column Reference Guide.md*)
* ✅ **Demo options**:

  * `query` for quick console demo
  * `batch` to generate a full **`out.xlsx`** for the judges

---

## Troubleshooting

* **`NameError: nullcontext`**
  Import was omitted in some Python builds. Add:

  ```python
  from contextlib import nullcontext
  ```
* **CUDA/VLLM issues**
  Make sure your CUDA driver matches the PyTorch/cu version. If you’re on a managed Linux box that needs CUDA libs:

  ```bash
  sudo apt-get update
  sudo apt-get install -y cuda-toolkit-12-8 libcurand-dev-12-8
  ```

  Or simply run with `--llm_backend llama-cpp` to stay CPU-only.
* **Slow/oversubscribed CPU**
  Try: `--blas_threads 1 --faiss_threads 1 --cpu_pool 8 --rows_concurrency 8`
* **Memory pressure while building**
  Use smaller batch: `--batch 256` and prefer `IVFPQ` index:

  ```bash
  --index ivfpq --ivf_nlist 16384 --pq_m 64 --pq_nbits 8
  ```
* **Fuzzy stage too slow**
  Lower the pool or worker count: `--fuzzy_workers 4` and/or reduce `AdvancedParams.fuzzy_pool` (see code defaults).

---

## Notes on data & licensing

* **SNOMED CT** is licensed; use only within terms permitted for your jurisdiction/institution.
* **RxNorm** is released by the U.S. National Library of Medicine; check their terms.
* This repo assumes you have permission to use the provided parquet files for the hackathon.

---

### Appendix: Example commands (copy/paste)

**Build**

```bash
python clean.py build \
  --snomed_parquet snomed_all_data.parquet \
  --rxnorm_parquet rxnorm_all_data.parquet \
  --out_dir indices
```

**Batch (LLM off)**

```bash
python clean.py batch \
  --index_dir indices \
  --in_file Test.xlsx --out_file out.xlsx \
  --use_llm_clean false --use_llm_rerank false \
  --llm_backend auto \
  --llm_concurrency 200 --rows_concurrency 128
```

**Batch (LLM on)**

```bash
python clean.py batch \
  --index_dir indices \
  --in_file Test.xlsx --out_file out.xlsx \
  --use_llm_clean true --use_llm_rerank true \
  --llm_backend auto \
  --llm_concurrency 200 --rows_concurrency 128
```

**Single query**

```bash
python clean.py query \
  --text "chest xr" \
  --entity_type "Procedure" \
  --index_dir indices \
  --use_llm_clean true --use_llm_rerank true \
  --llm_backend auto
```

---

## Acknowledgements

* FAISS, SentenceTransformers, RapidFuzz, vLLM, llama-cpp, PyTorch, and the maintainers of Qwen models.
* SNOMED CT and RxNorm teams for the vocabularies used in this challenge.

---

arch.md:
# Clinical Concept Harmonizer — Architecture Deep Dive

> Version: 2025-01 · Target Problem: HiLabs Hackathon 2025 · Repository entrypoint: `clean.py`

This document explains the system in **extreme detail**: goals, data model, build-time/indexing pipeline, query-time pipeline, algorithms, concurrency model, GPU/CPU execution plans, memory management, ranking theory, prompt engineering, configuration knobs, failure handling, performance characteristics, and extensibility. It is designed as a companion to the code in `clean.py`.

---

## 1) Goals, Non‑Goals, and Constraints

### Goals

* **High-accuracy clinical concept mapping** from messy inputs to **RxNorm** (meds) and **SNOMED CT** (everything else).
* **No external paid APIs**; use open-source models only.
* **Scalable** to millions of catalog rows with affordable compute (FAISS + compact embedding model).
* **Latency-friendly** for interactive queries **and** high-throughput for batch.
* **Deterministic fallback** when LLM is disabled (legacy ANN + fuzzy).

### Non‑Goals

* Not a general medical QA system; it maps short clinical strings to codes.
* Not a closed-loop ontology editor; the catalogs are provided as immutable inputs.

### Key Constraints

* Must run on **CPU-only** (using `llama-cpp`) and **GPU** (using `vLLM`).
* **Open-source only**: sentence transformers, faiss, rapidfuzz, vLLM, llama.cpp.
* **Reproducible** artifacts: indices + vector files + meta.

---

## 2) Data Model and Files

### Catalog schema (after preparation)

Each vocabulary (SNOMED, RxNorm) is normalized to the same internal schema:

| Column   | Role                                         |
| -------- | -------------------------------------------- |
| `row_id` | Stable integer id (0..N-1) for FAISS mapping |
| `CODE`   | String code in source vocabulary             |
| `STR`    | Human-readable term (name)                   |
| `CUI`    | UMLS concept id (if provided)                |
| `TTY`    | Term type (PT/SY/SCD/BN/…)                   |
| `STY`    | Semantic type string                         |
| `System` | `SNOMEDCT_US` or `RXNORM`                    |

### Build artifacts written to `--out_dir`

* `snomed_catalog.parquet`, `rxnorm_catalog.parquet`: canon catalogs.
* `snomed.index.faiss`, `rxnorm.index.faiss`: FAISS indices (HNSW/IVFPQ/Flat-IP).
* `snomed_vectors.f32`, `rxnorm_vectors.f32`: memory-mapped normalized embedding matrices (float32).
* `sty_vocab.json` + `sty_embeddings.npy`: vocabulary of STYs present across both catalogs and their embeddings.
* `meta.json`: complete reproducibility metadata (embedding model id, index type/params, paths, dim, etc.).

**Why memory-mapped vectors?**

* Reconstructing vectors from FAISS can be slow/imprecise depending on index type. Persisting the exact normalized embedding used at build time enables fast, vectorized, per-code aggregation later.

---

## 3) Build-Time Pipeline (Indexer)

**Entry:** `python clean.py build ...`

### Steps

1. **Load Parquet catalogs** (SNOMED, RxNorm) and call `prepare_catalog()` to project onto the common schema and assign `row_id`.
2. **Load Sentence Embedding model** (default: `google/embeddinggemma-300m`).
3. **Embed all `STR` values** in batches with GPU OOM backoff.
4. **Construct FAISS index** per system:

   * **HNSW Flat-IP**: default. Configurable `M`, `efConstruction`, `efSearch`.
   * **IVFPQ**: trains on a sample; lower memory, fast search at large scale.
   * **Flat-IP**: exact but memory/latency heavy.
5. **Add vectors with IDs** (`row_id`) and persist index files.
6. **Persist memory-mapped matrices** of the same vectors (`*.f32`).
7. **Embed STY vocabulary** once and store `sty_embeddings.npy` for quick semantic-type similarity.
8. **Write `meta.json`** to tie everything together.

### Similarity Metric

All embeddings are L2-normalized; we use **inner product** in FAISS which equals cosine similarity. We convert to [0,1] via `cos_to_01(x) = clip(0.5*(x+1), 0, 1)`.

---

## 4) Query-Time Pipeline — Two Modes

**Entry:** `python clean.py query ...` (single) or `batch` (multi-row).

We implement two matchers:

1. **Legacy ANN + Fuzzy** (**no LLM**): direct semantic search with optional RapidFuzz rerank; system preference (RxNorm for meds else SNOMED); thresholds and margins for switching.
2. **Advanced LLM-Assisted Pipeline** (**default**): multi-signal, multi-candidate reasoning with optional final LLM *code-level* rerank.

### 4.1 Legacy Pipeline (deterministic, low-cost)

* Embed query once; FAISS-search both indices.
* Optionally rerank each system’s topK using `rapidfuzz.token_set_ratio` against the original query.
* Pick **preferred system** (`choose_system(entity_type)`), but **switch** if alternative beats it by `--alt_margin` and passes `--min_score`.
* Return topK rows from the chosen system.

**When to use:** CPU-only, strict determinism, or when LLM is not desired.

### 4.2 Advanced LLM-Assisted Pipeline (default)

#### High-level idea

Use the LLM to **expand** the user input into multiple plausible **candidates** (synonyms/brands/abbreviations + short description + likely STYs). Then score codes using a **weighted mixture of signals** derived from description/keywords/direct match/STY similarity. Finally, **fuzzy re-rank**, **aggregate to per-code scores**, and optionally apply an **LLM reranker** on a small shortlist.

#### Signal Definitions

Let `q` be the raw query. For each catalog row `r` with text `STR_r` and STY `STY_r` we estimate:

* **Direct semantic**: `S_dir(r) = cos01(emb(q) · emb(STR_r))`
* **Description semantic** (from LLM): `S_desc(r) = max_c cos01(emb(desc_c) · emb(STR_r))`
* **Alt-keyword semantic**: `S_kw(r) = max_kw cos01(emb(kw) · emb(STR_r))`
* **STY compatibility**: `S_sty(r) = max_{sty_pred ∈ STY_pred} cos01(emb(sty_pred) · emb(STY_r))`

> `cos01(x) = clip(0.5*(x+1), 0, 1)` converts cosine to [0,1].

Then, the **row-level composite**

[
C(r) = w_{desc}·S_{desc}(r) + w_{kw}·S_{kw}(r) + w_{direct}·S_{dir}(r) + w_{sty}·S_{sty}(r)
]

with defaults `w_desc=0.30, w_kw=0.40, w_direct=0.20, w_sty=0.10` (tunable CLI flags).

#### Candidate Expansion (LLM)

* **System prompt** (`EXPAND_SYSTEM_PROMPT`) constrains output to **pure XML** of `<candidate>` blocks.
* For each candidate:

  * `<alternate_keywords>`: 2–10 short aliases/brands/abbreviations.
  * `<description>`: 1–3 sentence medical explanation.
  * `<possible_semantic_term_types>`: pick from the **provided STY list** only.
* Parser `extract_xml_candidates()` is defensive: handles missing fields, odd whitespace, single-block fallbacks.

#### Coarse Retrieval (FAISS)

For each signal family we perform **vector search** against **both** indices:

* Once for the **direct** query (`q`).
* Once per **candidate description**.
* **Batched** over all **alternate keywords** (much faster than per-kw loops).

Each hit contributes to its row’s `score_components` via the `cos01`-normalized score.

#### Fuzzy Re-rank (RapidFuzz)

We take the top **`fuzzy_pool`** rows by composite (default 500), then add a lexical check using **two-stage** RapidFuzz:

1. **Prefilter** with `ratio` on normalized strings for each anchor (query + best K keywords). Keep top-N indices union.
2. **Refine** candidates with `token_set_ratio` in parallel (`cdist`, `workers=-1` → all cores).

The `fuzzy` score is **not** blended with the composite numerically here; we simply **sort** by fuzzy for a **stability** pass and truncate to `top_pool_after_fuzzy` (default 250). This protects against semantic drift from embeddings when inputs are very short.

#### Per-Code Aggregation and Boost

Rows are bucketed by `(System, CODE)`. For each code we compute:

* `avg_all` = mean of **recomputed** composites across **all** rows of the code (using **memory-mapped vectors** for speed). Recompute uses the **best available** signals for the query (desc, kw, direct, STY).
* `avg_500` = mean of composites across only the rows that survived the fuzzy pool (`rows_top`).
* Let `p = 100 × |rows_top| / |rows_all|`. Compute a conservative **frequency boost**:

[
\text{boost}(p) = \sqrt{\log_{10}(\max(1.0001, p))}
]

Final **code score**:

[
F(\text{code}) = \text{avg_all} × \text{avg_500} × \text{boost}(p)
]

This favors codes that are **consistently strong** across their terms and have **good representation** in the top pool, mitigating outliers.

#### Final LLM Rerank (Optional)

* Build a small candidate list (default 30 codes) with **representative string** per code (prefers PT/FN/SCD via a rank map).
* Provide the LLM with:

  * Original query + entity type.
  * **Expanded understanding** summary (all candidates, their keywords and STYs).
  * Compact list `system | code | STY | TTY | STR` for each candidate.
* The LLM returns a **pure XML list** of `<choice>` with **only codes** and a short reason. We re-map codes back to best `(System, Code)` pair and **preserve at most `final_output_k`**.
* If LLM returns fewer items, we **fill** from the highest-scoring remaining codes.

---

## 5) LLM Backends and Auto‑Selection

Two interchangeable backends via `LLMConfig`:

* **vLLM** (GPU): loads HF weights (defaults to **Qwen3 4B** for smaller VRAM; **GPT‑OSS‑20B** otherwise) with optional quantization (`bitsandbytes`, `awq`, `gptq`, `mxfp4`).
* **llama-cpp** (CPU): loads **GGUF** (defaults to Unsloth Qwen3 4B Q4_K_XL). Threaded generation via `ThreadPoolExecutor`.

**Auto policy** (`pick_llm_backend_and_model`):

* If **no GPU** → `llama-cpp` with Qwen3 4B GGUF (downloaded from HF if needed).
* If GPU exists but **<22 GB per GPU** → **vLLM + Qwen3 4B**.
* Else → **vLLM + GPT‑OSS‑20B**.

LLM is used in two places only:

1. **Expansion** (keyword + desc + STY hints) — many parallel short prompts.
2. **Final rerank** — one prompt per query, small payload.

Both are **bounded** and run concurrently.

---

## 6) Concurrency Model and Threading

### Async Orchestration

* Batch mode creates an **async event loop** and limits shared resources using semaphores:

  * `_embed_sem`: throttles GPU-heavy embedding jobs (defaults to `2×num_gpus`, capped by `CPU_POOL`).
  * `_faiss_sem`: throttles FAISS searches (≈ `2×_embed_sem`).
* A configurable **row concurrency** (`--rows_concurrency`) controls how many records are processed in parallel at the pipeline level.

### LLM Concurrency

* **vLLM**: uses its own internal scheduler; we set `max_num_seqs` and enable **prefix caching** to exploit identical system prompts.
* **llama-cpp**: the engine is not thread-safe for concurrent calls; we serialize each chat completion via an **async lock**, but run the blocking call in a **thread pool** sized by `LLMConfig.concurrency`.

### CPU Thread Control (`configure_cpu_threading`)

* Sets FAISS **OpenMP** threads via `faiss.omp_set_num_threads`.
* Caps BLAS backends via `OMP_NUM_THREADS`, `OPENBLAS_NUM_THREADS`, `MKL_NUM_THREADS`, `BLIS_NUM_THREADS`, `NUMEXPR_NUM_THREADS`.
* Optionally uses `threadpool_limits` context to bound NumPy/Scipy and friends during heavy sections.

### Why two levels of concurrency?

* **Pipeline level** (rows): hides per-row variability and IO.
* **Component level** (emb/FAISS/LLM): prevents head-of-line blocking and GPU OOM while keeping hardware busy.

---

## 7) Memory Management and OOM Resilience

* Embedding uses **backoff batching** on CUDA OOM: halving batch size until it fits (down to 1).
* `clear_cuda_memory()` aggressively frees caches and triggers GC between phases.
* `model_roundtrip_cpu_gpu()` can be used as a VRAM defragmentation nudge.
* FAISS indices live on **CPU RAM**; vectors are normalized **once** and stored on disk as **memmaps** for later **vectorized** scoring without rebuilding.

---

## 8) Prompt Engineering and XML Parsing

### Expansion Prompt

* Strongly constrained to **ASCII XML** with only three tags. We provide the **finite list of allowed STYs** so the model cannot invent new labels.
* We encourage **multiple candidates** when the input is ambiguous (e.g., "XR chest" → imaging vs. order).

### Rerank Prompt

* Positions the LLM as an adjudicator over a **shortlist of standardized codes**. The model does **not invent new codes**—it only ranks provided ones.
* Output again is pure XML to simplify parsing.

### Parsers

* `extract_xml_candidates()`/`extract_xml_choices()` use robust regex splits with graceful fallbacks (single-block or partial field missing → skip/empty lists).

---

## 9) Algorithmic Complexity & Performance Notes

Let `N_sno`, `N_rx` be catalog sizes, `d` embedding dim.

* **Build time**: `O((N_sno+N_rx) × cost(embed)) + O(N log N)` (index construction), dominated by embedding.
* **Query time** (per row):

  * Embedding: `O(#texts × d)`; typically small (query + 0–10 KWs + ≤1 desc).
  * FAISS search: `O(log N)` average for HNSW; `O(d × efSearch)` tuned by `efSearch`.
  * Fuzzy cdist: `O(A × M)` where `A` anchors (≤9) and `M` reduced set (≤prefilter). Leveraging C++ + multithread keeps it small.
  * Aggregation uses memmap **matrix multiplications** for each code (vectorized in NumPy): highly efficient.

**Empirical**

* Build: ~11 min on 1×H100, 30–60 min on T4-class GPUs; ~10GB disk for artifacts (depends on index type).
* Batch LLM-off: seconds to sub-minute for typical test sheets.
* Batch LLM-on: depends on GPU and concurrency settings; designed to scale linearly with hardware.

---

## 10) Configuration Knobs (Cheatsheet)

### Build

* `--model`: sentence embedding model id.
* `--index`: `hnsw|ivfpq|flat` and their params (`--hnsw_m`, `--hnsw_efc`, `--hnsw_efs`, `--ivf_nlist`, `--pq_m`, `--pq_nbits`).
* `--batch`: embedding batch size.

### Query/Batch (Core)

* `--use_llm_clean`, `--use_llm_rerank`: toggle LLM stages.
* `--topk`: final outputs per row.
* `--rerank`: `none|rapidfuzz` legacy path option.
* `--min_score`, `--alt_margin`: legacy system-switch policy.

### Weights and Pools

* `--weights_desc`, `--weights_kw`, `--weights_direct`, `--weights_sty`.
* Advanced params (compiled): `semantic_top_desc`, `semantic_top_kw_each`, `fuzzy_pool`, `top_pool_after_fuzzy`, `final_llm_candidates`.

### LLM Backend

* `--llm_backend`: `auto|vllm|llama-cpp`.
* `--llm_hf_model_id`, `--llm_gguf_path`.
* Generation: `--llm_max_new_tokens`, `--llm_temperature`, `--llm_top_p`.
* Scaling: `--llm_concurrency`, `--llm_tp`, `--llm_n_threads`, `--vllm_quantization`.

### CPU/Threads

* `--blas_threads`, `--faiss_threads`, `--cpu_pool`, `--fuzzy_workers`.
* Batch: `--rows_concurrency`.

---

## 11) Failure Modes & Mitigations

| Risk                   | Symptom                           | Mitigation                                                                        |
| ---------------------- | --------------------------------- | --------------------------------------------------------------------------------- |
| CUDA OOM               | Embedding crashes                 | backoff batches; clear CUDA cache; lower `--batch`; use CPU or smaller model      |
| vLLM VRAM limits       | Engine fails to initialize        | auto-select Qwen4B; set `--vllm_quantization` to `mxfp4`/`awq`; reduce `--llm_tp` |
| llama-cpp contention   | Random slowdowns                  | serialize access (`_lock`); adjust `--llm_concurrency`; increase `--n_threads`    |
| Over-threading         | CPU pegged with little throughput | set `--blas_threads`, `--faiss_threads`, `--cpu_pool` sanely                      |
| Bad LLM XML            | No candidates/choices parsed      | robust regex + fallbacks; pipeline still returns **non-LLM** results              |
| Ambiguous short inputs | Wrong concept                     | rely on multi-candidate expansion + fuzzy; consider raising `w_kw` temporarily    |

---

## 12) Security, Privacy, and Reproducibility

* **Data never leaves the machine**: no remote API calls; only HF hub downloads of **open** model weights (optional token for rate limits).
* **Reproducibility**: `meta.json` captures model id + dims + index params + filenames. Artifacts are pure files—no hidden state.
* **Determinism**: Legacy path is deterministic; LLM path can vary slightly across seeds/hardware; final lists are constrained to existing codes.

---

## 13) Extensibility

* **Add a new vocabulary**: prepare same 7-column schema; train/embed; add new FAISS index + memmap; extend `choose_system()` policy.
* **New signal**: add term-level signal and weight in `AdvancedWeights` + accumulation in `score_components`.
* **Alternate embedding model**: update `--model` at build; re-build indices and STY embeddings.
* **Richer rerank**: swap RapidFuzz scorer or combine fuzzy numerically in the composite if desired.

---

## 14) Worked Example (End-to-End)

Input row:

```
Input Entity Description: "chest xr"
Entity Type: "Procedure"
```

1. **LLM expansion** → candidates: {"chest x-ray", "cxr", "radiograph chest"}, desc ≈ “plain radiograph of the chest”, STY hints "Diagnostic Procedure".
2. **Semantic searches**: direct on "chest xr"; desc search; batched kw searches.
3. **Row composites** built across both indices.
4. **Fuzzy** stage boosts strings containing both tokens {"chest", "x-ray"}.
5. **Aggregate** per code: many SNOMED rows share X-ray procedures; `avg_all` and `avg_500` select the consistent code; frequency boost favors codes with multiple supporting synonyms.
6. **LLM rerank** (optional): chooses the best procedure code and explains briefly.

---

## 15) Pseudocode (Advanced Matcher)

```text
exp_cands = LLM.expand(q, entity_type)
q_vec = emb(q)
for cand in exp_cands:
  d_vec = emb(cand.description)
  kw_vecs = emb(cand.keywords)
  for sys in {SNOMED,RXNORM}:
    add_scores(search(sys, d_vec), "desc")
    add_scores_batch(search(sys, kw_vecs), "kw")
  add_scores(search(SNOMED, q_vec), "direct")
  add_scores(search(RXNORM, q_vec), "direct")
  apply_sty_map(predicted_stys)

rows = top_by_composite(fuzzy_pool)
rows = fuzzy_resort_and_truncate(rows, top_pool_after_fuzzy)

codes = group_by_code(rows)
for code in codes:
  all_vecs = memmap_vectors(code)
  recompute composites vs {q_vec, d_vec, kw_vecs, sty_map}
  avg_all, avg_500, p = reduce(code)
  score = avg_all * avg_500 * sqrt(log10(max(1.0001,p)))

shortlist = topN(codes)
if use_llm_rerank:
  final = LLM.rerank(shortlist, context)
else:
  final = shortlist[:K]
```

---

## 16) Known Limitations & Future Work

* **Unit handling**: current system treats dosage/quantities lexically; a dose parser could improve RxNorm precision.
* **Negation/qualifiers**: phrases like “rule out pneumonia” may map too strongly to the disease; adding a **clinical negation** detector would help.
* **Context window**: single-field input; future: multi-column fusion (age/sex/lab units) for disambiguation.
* **Learning-to-rank**: the score weights are fixed; we could learn them from labeled data.
* **Cache**: response caching (query→result) for repeats in batch runs.

---

## 17) Practical Tuning Recipes

* **CPU-only laptop**: `--use_llm_clean false --use_llm_rerank false --rerank rapidfuzz --blas_threads 4 --faiss_threads 4`.
* **Single 8–12GB GPU**: `--llm_backend auto --vllm_quantization mxfp4 --llm_concurrency 64 --rows_concurrency 64`.
* **Accuracy-first**: increase `w_kw` and `w_desc`; ensure LLM stages enabled; raise `final_llm_candidates` to 50.
* **Throughput-first**: set `use_llm_rerank=false`; keep expansion on; reduce `semantic_top_*` and `fuzzy_pool`.

---

## 18) CLI Quick Reference

### Build

```bash
python clean.py build \
  --snomed_parquet snomed_all_data.parquet \
  --rxnorm_parquet rxnorm_all_data.parquet \
  --out_dir indices \
  --model google/embeddinggemma-300m
```

### Batch (LLM on)

```bash
python clean.py batch \
  --index_dir indices \
  --in_file Test.xlsx --out_file out.xlsx \
  --use_llm_clean true --use_llm_rerank true \
  --llm_backend auto \
  --llm_concurrency 200 --rows_concurrency 128
```

### Batch (LLM off)

```bash
python clean.py batch \
  --index_dir indices \
  --in_file Test.xlsx --out_file out.xlsx \
  --use_llm_clean false --use_llm_rerank false \
  --rerank rapidfuzz
```

### Single Query

```bash
python clean.py query \
  --index_dir indices \
  --text "hcv rna" \
  --entity_type "Procedure" \
  --use_llm_clean false --use_llm_rerank false
```

---

## 19) File/Module Map

* **`clean.py`** — all logic: build, load, query, batch; FAISS, embeddings, LLM adapters, fuzzy, scoring, CLI.
* **Artifacts in `indices/`** — indices, memmaps, STY embeddings, meta.
* **`Test.xlsx`** — input; **`out.xlsx`** — output with columns: system, code, description, STY, TTY (+ optional topK columns).

---

## 20) Summary

This architecture **blends** fast vector search, lexical sanity checks, semantic-type priors, and *constrained* LLM reasoning. It is:

* **Accurate** (multi-signal evidence + optional LLM adjudication),
* **Scalable** (FAISS + batched embeddings + memmaps),
* **Portable** (GPU/CPU), and
* **Reproducible** (file artifacts + meta).

The system is purpose-built for the Hackathon’s harmonization task while remaining extensible for real-world production use.


detailed explanation of everything by another ai:
# Clinical Concept Harmonizer — Architecture (arch.md)

**Version:** Jan 2025
**Owner:** HiLabs
**Scope:** RxNorm + SNOMED CT concept matching with LLM-assisted expansion & ranking

---

## 1) Executive summary

This service normalizes messy clinical text (medications, labs, diagnoses, procedures) into standardized codes from **RxNorm** and **SNOMED CT**. It combines:

* **Dense retrieval** using a Sentence-Transformer embedding model and FAISS indices (HNSW / IVFPQ / Flat-IP).
* **LLM-assisted query expansion** (alternate keywords, clarifying description, candidate semantic types) and **optional LLM re‑ranking** of final codes.
* **Multi‑signal scoring** that blends: description match, keyword match, direct query match, and **STY** (Semantic Type) similarity.
* **Fuzzy matching** (RapidFuzz) to re-rank a large pool before per‑code aggregation and final selection.
* **High‑throughput batch pipeline** with asyncio + thread pools; CPU/GPU aware; memory‑mapped vectors; careful CUDA hygiene.

The result is a robust, scalable matcher that outperforms pure string or pure embedding baselines, while keeping the legacy non‑LLM path fully available.

---

## 2) High‑level dataflow

```
┌────────────┐      ┌──────────────────────┐      ┌──────────────────────┐
│  Raw Text  │ ───▶ │  LLM Expansion       │ ───▶ │  Semantic Retrieval   │
│ (query,    │      │  (keywords, desc,    │      │  (FAISS over Rx/SN)   │
│ entity type)│     │   predicted STYs)    │      │  + multi-signal scores│
└────────────┘      └──────────────────────┘      └────────┬─────────────┘
                                                            │
                                                            ▼
                                                ┌─────────────────────────┐
                                                │ Fuzzy Re-rank (RapidFuzz)│
                                                └────────┬────────────────┘
                                                         │ (top pool)
                                                         ▼
                                            ┌─────────────────────────────┐
                                            │ Per‑code aggregation & boost │
                                            └────────┬────────────────────┘
                                                     │
                                                     ▼
                                   ┌───────────────────────────────────────────┐
                                   │ Optional LLM final re‑rank of top codes   │
                                   └───────────────────────────────────────────┘
                                                     │
                                                     ▼
                                             Final ranked code(s)
```

**Build path (one‑time per catalog):**

```
Parquet catalogs ─▶ catalog prep ─▶ batched embedding ─▶ FAISS index write
                                     └─▶ STY vocab + embeddings (npy/json)
```

---

## 3) Files & artifacts

During **build** (`build_indices`):

* `snomed.index.faiss`, `rxnorm.index.faiss` — FAISS indices.
* `snomed_catalog.parquet`, `rxnorm_catalog.parquet` — slimmed catalogs with columns
  `[row_id, CODE, STR, CUI, TTY, STY, System]`.
* `snomed_vectors.f32`, `rxnorm_vectors.f32` — memory‑mapped float32 matrices (normalized embeddings, row‑aligned).
* `sty_vocab.json` — unique STY inventory across both systems.
* `sty_embeddings.npy` — normalized embeddings for each STY token for fast similarity.
* `meta.json` — model + index metadata.

At **query/batch** time these are loaded via `load_bundle(...)` and kept in memory/memmap for high throughput.

---

## 4) Core modules & responsibilities

### 4.1 Device, memory, and threading

* `gpu_summary`, `clear_cuda_memory` — detect GPUs, report VRAM; aggressively clear caches after large phases.
* `configure_cpu_threading` — caps BLAS/FAISS threads; integrates with `threadpoolctl` via a context manager.
* `default_concurrency` — scales with GPU count or CPU cores.

### 4.2 Embeddings & FAISS

* `load_model` / `embed_texts` — loads SentenceTransformer model; OOM‑resilient batching with backoff; always normalizes.
* Index builders: `build_hnsw`, `build_ivfpq`, `build_flat` (all IP/cosine‑equivalent on normalized vectors).
* `prepare_catalog` — trims and normalizes catalog columns; assigns stable `row_id`.
* `train_ivfpq_if_needed` — trains IVF‑PQ on a sample of code strings.
* `faiss_search`, `reconstruct_vec` — light wrappers for search and vector recon.

### 4.3 LLM backends

* `pick_llm_backend_and_model` selects **vLLM** (GPU) or **llama.cpp** (CPU/GGUF) automatically.
* **Sync**: `LLMClient` (vLLM or llama.cpp) for single‑query mode.
* **Async**: `AsyncLLMClient` (vLLM async engine or llama.cpp + thread pool) for batch throughput.
* Quantization defaults (vLLM): `bitsandbytes`. GGUF path is used only for llama.cpp.
* Concurrency control: semaphore in `AsyncLLMClient`, plus global thread‑pool for CPU work.

### 4.4 Prompts & XML parsing

* **Expansion prompts** (`EXPAND_*`) produce XML `<candidate>` blocks with:
  `alternate_keywords`, `description`, `possible_semantic_term_types`.
* **Rerank prompts** (`RERANK_*`) accept a candidate blob and return ordered `<choice>` codes with reasons.
* Parsers: `extract_xml_candidates`, `extract_xml_choices` — robust to minor formatting issues, fall back to simple regex.

### 4.5 Scoring & ranking

* **Signals:**

  * `desc` — cosine match to LLM description.
  * `kw` — max cosine over alternate keywords.
  * `direct` — cosine to the raw input query.
  * `sty` — STY similarity via pre‑embedded STY vocabulary.
* **Weights:** `AdvancedWeights(w_desc=.30, w_kw=.40, w_direct=.20, w_sty=.10)` (CLI‑tunable).
* **Fuzzy:** `fuzzy_scores_max` uses RapidFuzz `ratio` prefilter + `token_set_ratio` refine across anchors `{query} ∪ top‑KWs`.
* **Aggregation:** collapse rows to **per‑code**

  * Recompute composites using memmapped vectors for all rows of a code.
  * Compute `avg_all` and `avg_500` (the average on the top rows bucket) and
    boost by `sqrt(log10(percentage_in_top_pool))`.
* **Final selection:** take top‑N by aggregated score, optional LLM re‑rank (codes only) to respect system preferences.

### 4.6 Pipelines

* **Legacy (non‑LLM)**: single embedding search per system, optional RapidFuzz rerank, preference heuristic chooses RxNorm vs SNOMED.
* **Advanced (LLM)**: implemented in `advanced_match_one` and `advanced_match_one_async`.

  * Incorporates query expansion, multi‑signal retrieval, fuzzy re‑rank, code aggregation, optional LLM final re‑rank.

### 4.7 CLI & modes

* `build`: construct indices & STY artifacts.
* `query`: single query; tabular printout of top‑k.
* `batch`: high‑throughput async processor with progress bar, `include_topk` columns, and CSV/XLSX output.

---

## 5) Detailed sequence diagrams

### 5.1 `build` (index creation)

1. `maybe_hf_login` if token passed.
2. Load embedding model; read full Parquet catalogs.
3. `prepare_catalog` → standardized tables with `row_id`.
4. Allocate memmaps: `*.f32` of shape `[n_rows, dim]` per system.
5. For each system:

   * Construct index (`hnsw` | `ivfpq` | `flat`). Train IVF‑PQ if selected.
   * In **batches**: embed `STR` → add vectors with **given row_ids**; persist vectors to memmap.
   * Set HNSW `efSearch` and write FAISS index.
6. Build **STY artifacts**: unique STY list; embed to `sty_embeddings.npy` (normalized).
7. Persist `meta.json` with all knobs (model, dim, index params, file names).

### 5.2 `query` (advanced path)

1. Load bundle (indices, cats, memmaps, STYs, model). Configure HNSW `efSearch` and CPU threads.
2. Initialize LLM backend (`LLMClient`) if any LLM knobs are enabled.
3. LLM **expansion** → candidates (alt keywords, description, predicted STYs). If disabled, use a degenerate candidate.
4. Compute `q_vec` for original text.
5. For each candidate:

   * Embed `description` → retrieve top‑K (desc‑signal) on both systems.
   * Embed all `keywords` (batched) → retrieve (kw‑signal) on both systems.
   * Run direct retrieval with `q_vec` (direct‑signal) on both systems.
   * Compute **STY map** once via vector ops: `max cos(pred_STYs, sty_vocab)`; assign to any seen row’s STY.
6. Compute **composite** per row using weights; take **fuzzy pool** (e.g., top‑500 by composite).
7. **Fuzzy re‑rank** the pool using RapidFuzz (`ratio` prefilter → `token_set_ratio` refine), then keep top‑N (e.g., 250).
8. **Per‑code aggregation**:

   * For each code present in top pool, collect **all rows** for that code.
   * Recompute composites **vectorized** from memmapped vectors against `{q_vec, desc_vec?, kw_vecs?, sty_map}`.
   * Produce `final = avg_all * avg_500 * sqrt(log10(%_in_pool))` and rank codes.
9. Optionally **LLM re‑rank** the top codes with terse justifications.
10. Emit final top‑k rows (best representation per code favoring TTY `PT` > `FN/SCD/SBD` > others).

### 5.3 `batch`

* Same as `query`, but drives **`advanced_match_one_async`** under an async runner:

  * Global CPU thread pool (`--cpu_pool`) for `to_thread` work.
  * LLM concurrency semaphore (`--llm_concurrency`).
  * Semaphores for embedding and FAISS async wrappers.
  * Row‑level concurrency (`--rows_concurrency`).

---

## 6) Key algorithms & formulas

### 6.1 Cosine→[0,1]

All cosine scores are normalized to `[0,1]` via `cos_to_01(x) = clip(0.5*(x+1), 0, 1)` to unify metrics.

### 6.2 Composite per row

```
composite_row = w_desc*desc + w_kw*kw + w_direct*direct + w_sty*sty
```

Signals are **max‑pooled** across multiple candidate expansions.

### 6.3 Fuzzy re‑rank

* Anchors = `{normalized(query)} ∪ top-K normalized(alt_keywords)` (deduped).
* Stage 1: parallel `ratio` prefilter (keeps top‑M per anchor).
* Stage 2: parallel `token_set_ratio` on union of kept indices.
* Final fuzzy score = max across anchors, normalized to `[0,1]`.

### 6.4 Per‑code aggregation & boost

For each code:

* Recompute composites for **every row** of the code (vectorized from memmap).
* `avg_all` = mean composite over all rows; `avg_500` = mean over rows that landed in the fuzzy pool; `%_in_pool` = |rows_top|/|rows_all|.
* Final score: `final = avg_all * avg_500 * sqrt(log10(max(1.0001, %_in_pool)))`.

  * Intuition: combine overall semantic match, top‑pool concentration, and mild boost for codes with many high rows.

---

## 7) LLM integration details

### 7.1 Backend selection

* **Auto** logic (`pick_llm_backend_and_model`):

  * If **no GPU** → `llama.cpp` with Qwen3‑4B‑Instruct GGUF.
  * If GPU present with **<22 GB VRAM per GPU** → vLLM with Qwen3 4B HF (bnb quant).
  * Else prefer larger “OSS‑20B” HF (bnb quant) for richer expansions.

### 7.2 Prompts (summary)

* **Expand (system/user)**: produce 1–5 `<candidate>` blocks with alt keywords, a short medical description, and predicted STYs (from the enumerated **sty_inventory**).
* **Re‑rank (system/user)**: input is original query, entity type, the expansion summary, and a **candidate blob** (system|code|STY|TTY|name). Output is up to `K` `<choice>` blocks with `<code>` and `<reasoning>`.

### 7.3 Robustness

* XML is parsed with flexible regex; if the LLM returns malformed XML, we still salvage content.
* Temperature defaults to `0.1` for stability; `top_p=0.9`; `max_new_tokens=512`.
* vLLM enables prefix caching to amortize common system prompt costs.

---

## 8) Performance, capacity, and scaling

### 8.1 Index size & memory

* Vectors are **float32** (normalized). With `dim≈384–768`, each million rows is ~1.5–3.0 GB. Memmaps stream from disk; RAM pressure is limited.
* HNSW typically offers the best query latency/recall trade‑off. IVF‑PQ enables large catalogs with smaller footprints at a minor recall cost.

### 8.2 Throughput knobs

* `--blas_threads`, `--faiss_threads` tune CPU parallelism.
* `--fuzzy_workers` (`RapidFuzz`) uses native parallel `cdist`.
* `--llm_concurrency`, `--rows_concurrency` control LLM and row‑level parallelism.
* vLLM: `--llm_tp` tensor parallel = number of GPUs; quantization reduces VRAM.

### 8.3 CUDA hygiene

* `clear_cuda_memory` and **model round‑trip** to defragment caches between long phases.
* Embedding loop backs off batch size upon OOM.

---

## 9) Reliability and failure modes

* **Absent RapidFuzz**: pipeline skips fuzzy scores gracefully.
* **Malformed LLM XML**: parsers degrade to single‑block capture; pipeline continues.
* **Low recall** on one system: dual‑system search + per‑code aggregation mitigates; final LLM re‑rank can prefer the more appropriate system.
* **Determinism**: set temperature low (default) and seed external components if absolute reproducibility is required. Exact tie‑breaking across HNSW/IVF can be non‑deterministic.
* **Legacy fallback**: disable `--use_llm_clean/--use_llm_rerank` for pure ANN + RapidFuzz.

---

## 10) Security & privacy

* No PHI is persisted; inputs are processed in‑memory.
* Indices contain **public catalog strings** and codes only.
* If using **hosted** LLMs, ensure data handling complies with org policies (vLLM is local by default).
* SNOMED CT licensing considerations apply depending on deployment region.

---

## 11) Extensibility roadmap

* **New vocabularies** (ICD‑10‑CM, LOINC) by adding catalogs, building indices, and mapping STY/TTY analogs.
* **Additional signals** (e.g., code priors, usage frequency, site‑specific whitelists/blacklists).
* **Caching** expansions per query canonicalization to reduce LLM calls.
* **Better XML schema** (JSON is simpler) + schema validation.
* **RAG over documentation** to improve expansion descriptions.

---

## 12) CLI: key parameters (cheat sheet)

**Build**

* `--model`: embedding model (default `google/embeddinggemma-300m`).
* `--index`: `hnsw` | `ivfpq` | `flat` (+ HNSW/IVF/PQ knobs).
* `--batch`: embedding batch size.

**Query/Batch**

* `--use_llm_clean`, `--use_llm_rerank` (true by default).
* `--llm_backend`: `auto` | `vllm` | `llama-cpp`.
* `--vllm_quantization`: quantization method (default `bitsandbytes`).
* Weights: `--weights_desc/kw/direct/sty`.
* Fuzzy: `--fuzzy_workers`, `--rerank=rapidfuzz|none`.
* CPU: `--blas_threads`, `--faiss_threads`, `--cpu_pool`.

---

## 13) Deep dive: important functions/classes

* **Embedding**

  * `embed_texts(model, texts, batch_size, normalize)` — backoff loop handles CUDA OOM; always returns `float32`.
* **Async wrappers** (`*_async`) with semaphores for fair scheduling.
* **LLM**

  * `AsyncLLMClient.generate_one` and `.generate_many` — vLLM streaming/awaitable support; llama.cpp guarded by a lock.
  * `LLMClient.generate` — sync helper for single queries (simpler flow).
* **Advanced pipeline**

  * `advanced_match_one` / `advanced_match_one_async` — orchestrate expansion → retrieval → fuzzy → per‑code aggregation → optional LLM rerank.
* **Aggregation helpers**

  * `best_name_for_code` prefers TTY `PT` over synonyms or forms (`FN`, `SCD`, `SBD`).
  * `precompute_sty_scores_map` computes vectorized max similarity for predicted STYs against the full STY vocab.

---

## 14) Q&A prep (likely questions + crisp answers)

**Q1. Why combine embeddings, fuzzy, and LLMs?**
No single signal dominates across all noise types. Embeddings capture semantics, fuzzy captures orthography/abbreviations, LLM expansion injects domain knowledge (brand names, lay terms). The weighted fusion improves recall and precision.

**Q2. How do you keep RxNorm preferred for medications?**
We use the input **entity type** to bias selection and, in the LLM rerank prompt, explicitly say “prefer RxNorm for meds; SNOMED otherwise”. The legacy path also picks the preferred system unless the alternative is strongly better.

**Q3. What’s the purpose of per‑code aggregation?**
Catalogs contain many rows per code (synonyms, forms). Aggregating stabilizes rankings by averaging across all variants and leveraging how many of a code’s strings surfaced in the top pool.

**Q4. What are the main performance levers?**
HNSW efSearch (latency/recall), IVF‑PQ params (footprint/recall), embedding batch size, FAISS/BLAS threads, RapidFuzz workers, and LLM concurrency. vLLM prefix caching yields large savings for repeated prompts.

**Q5. How do you avoid LLM hallucination?**
LLM output only **proposes** candidates and optionally re‑orders the **already valid** codes. Codes always originate from FAISS catalogs; the LLM never fabricates unseen codes.

**Q6. Determinism and reproducibility?**
Set temperature low (0.0–0.1), fix seeds where possible, and pin index params. HNSW/IVF have minor non‑determinism; within acceptable tolerance for ranking tasks.

**Q7. Failure when RapidFuzz is missing?**
We skip fuzzy scoring; the pipeline still runs with dense retrieval and STY signals.

**Q8. Why use STY embeddings instead of exact matches?**
LLM predicts **likely** STYs, not certainties. Embedding similarity soft‑matches related STYs (e.g., “Clinical Drug” vs “Pharmacologic Substance”), improving robustness.

**Q9. How big should the fuzzy/top pools be?**
Defaults (`fuzzy_pool=500`, `top_pool_after_fuzzy=250`) balance recall and cost. Increase for noisier inputs or very large catalogs.

**Q10. Can we add ICD‑10‑CM or LOINC?**
Yes—add catalogs, build indices, and (optionally) extend STY inventory. The pipeline is vocabulary‑agnostic.

**Q11. What are the GPU requirements?**
Any NVIDIA GPU works. Per‑GPU VRAM ≥ 22 GB allows larger models; otherwise the auto‑selector uses Qwen 4B HF + quant. The ANN/embedding stage is light (few hundred MBs) for common ST embedding models.

**Q12. How are inputs/outputs structured for batch?**
Input table must contain `"Input Entity Description"` and `"Entity Type"`. Output includes top‑1 columns by default, plus `include_topk` to expand to `k` alternatives.

**Q13. How do you handle nullcontext error?**
`from contextlib import nullcontext` is imported and used as a fallback when `threadpool_limits` is absent. (Fixed in code.)

**Q14. What does the boost `sqrt(log10(%))` achieve?**
Dampened lift for codes with many strong rows in the pool without letting row‑rich codes dominate purely by volume.

**Q15. Where does latency go?**
Typically: LLM expansion (if enabled) ≫ fuzzy re‑rank ≈ ANN search ≫ aggregation. Tuning anchors and concurrency gives the biggest wins.

---

## 15) Operational guidance

* Prefer building indices offline; commit the whole `out_dir` as a versioned artifact (immutable).
* Start with **legacy path** in production; enable LLM expansion for tricky cohorts and A/B compare.
* Log request IDs around LLM calls; capture prompt + hashes, not PHI.
* Export top‑k with reasons for human review in early rollouts.

---

## 16) Appendix

### 16.1 Defaults (selected)

* Weights: `desc=.30, kw=.40, direct=.20, sty=.10`
* Pools: `semantic_top_desc=500, semantic_top_direct=500, semantic_top_kw_each=100, fuzzy_pool=500, top_pool_after_fuzzy=250`
* LLM: `max_new_tokens=512, temperature=0.1, top_p=0.9`

### 16.2 Environment

* `PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True` (helps fragmentation)
* BLAS pool caps via `OMP_NUM_THREADS`, `OPENBLAS_NUM_THREADS`, `MKL_NUM_THREADS`, `BLIS_NUM_THREADS`, `NUMEXPR_NUM_THREADS` (set by code if `--blas_threads` is used).

### 16.3 Important columns

* **TTY** (term type): `PT` (preferred term), `FN` (fully specified name), `SCD/SBD` (RxNorm clinical/brand dose forms), etc.
* **STY** (semantic type): curated types used to bias ranking; embedded to enable soft similarity.

### 16.4 Third‑party deps

* FAISS, Sentence-Transformers, PyTorch, RapidFuzz, vLLM/llama.cpp, HuggingFace Hub utilities, pandas, numpy, tqdm.

---

**End of document.**


flow mermaid codes:
%% 4) Batch flow (batch command) – Async rows + shared LLM/ANN
flowchart TD
  A[run_batch] --> B[load_bundle + set hnsw efSearch]
  A --> C[configure_cpu_threading + threadpool_limits]
  A --> D[read_input_table - in_file]
  A --> E{use_llm_*?}

  E -- yes --> F[Init AsyncLLMClient - concurrency, TP, quant]
  E -- no --> F0[No LLM path]

  %% Orchestration
  subgraph ORCH[Async Orchestrator]
    direction TB
    G[ThreadPoolExecutor size = cpu_pool] --> H[asyncio Queue of row indices]
    H --> I[rows_concurrency workers]
  end

  F --> ORCH
  F0 --> ORCH

  %% Worker
  subgraph W[process_row - i]
    direction TB
    W1[Read: Input Entity Description, Entity Type]
    W2{use_llm_*?}
    W2 -- yes --> W3[advanced_match_one_async - LLM expand → ANN → fuzzy → per-code → optional LLM rerank]
    W2 -- no --> W4[Legacy ANN → choose_system_hits]
    W3 --> W5[Write top1 + optional topk cols into DF]
    W4 --> W5
  end

  I --> W1
  W5 --> J[write_output_table - out_file]

%% 2) Build flow (build command)
flowchart TD
  A[build_indices] --> B[Load SNOMED parquet]
  A --> C[Load RxNorm parquet]
  B --> D[prepare_catalog SNOMED]
  C --> E[prepare_catalog RxNorm]
  D --> F[Init embed model]
  E --> F
  F --> G[determine dim + device]

  %% Index factory
  G --> H{index type?}
  H -->|hnsw| H1[faiss.IndexHNSWFlat d, M set efConstruction]
  H -->|ivfpq| H2[faiss.IndexIVFPQ train_ivfpq_if_needed]
  H -->|flat| H3[faiss.IndexFlatIP]
  H1 --> I1[wrap with IndexIDMap2]
  H2 --> I1
  H3 --> I1

  %% SNOMED build
  I1 --> J1[Loop SNOMED rows in batches embed_texts -> add_with_ids]
  J1 --> K1[Set hnsw efSearch if hnsw]
  K1 --> L1[write_index snomed.index.faiss]
  J1 --> M1[memmap write snomed_vectors.f32]

  %% Barrier + RX build
  L1 --> N[clear_cuda_memory + model_roundtrip_cpu_gpu]
  M1 --> N
  N --> I2[Clone index factory for RxNorm]
  I2 --> J2[Loop RxNorm rows in batches embed_texts -> add_with_ids]
  J2 --> K2[Set hnsw efSearch if hnsw]
  K2 --> L2[write_index rxnorm.index.faiss]
  J2 --> M2[memmap write rxnorm_vectors.f32]

  %% STY embeddings + meta
  L2 --> P[Build STY vocab as union of STY from both catalogs]
  M2 --> P
  P --> Q[embed_texts STY list -> sty_embeddings.npy]
  Q --> R[write sty_vocab.json]
  R --> S[write meta.json model, dim, files, index params]

%% 1) High-level system overview
flowchart TB
  %% Entrypoint
  A["argparse CLI (main -> subparsers)"] -->|build| B(Build Indices)
  A -->|query| C(Query Single)
  A -->|batch| D(Batch Process)

  %% Storage
  subgraph S["On-disk Artifacts"]
    S1[snomed_catalog.parquet]
    S2[rxnorm_catalog.parquet]
    S3[snomed.index.faiss]
    S4[rxnorm.index.faiss]
    S5[snomed_vectors_f32_rx_memmap]
    S6[rxnorm_vectors_f32_rx_memmap]
    S7[sty_vocab.json]
    S8[sty_embeddings.npy]
    S9[meta.json]
  end

  B -- write --> S1 & S2 & S3 & S4 & S5 & S6 & S7 & S8 & S9

  %% Runtime bundle
  subgraph L["Runtime Bundle (load_bundle)"]
    L1[SentenceTransformer model]
    L2[FAISS snomed index]
    L3[FAISS rxnorm index]
    L4[snomed catalog DF]
    L5[rxnorm catalog DF]
    L6[STY vocab + embeddings]
    L7[memmap views of vectors]
  end

  C -- load --> L
  D -- load --> L

  %% LLM control plane
  subgraph X["LLM Control Plane"]
    X1[pick_llm_backend_and_model]
    X2{Backend?}
    X3[vLLM engine: quant, TP, concurrency]
    X4[llama.cpp engine: n_threads, n_ctx]
  end

  C -- if use_llm_* --> X1 --> X2
  D -- if use_llm_* --> X1
  X2 -->|vllm| X3
  X2 -->|llama-cpp| X4

  %% Outputs
  C --> O1[stdout: Top-k codes]
  D --> O2[out_file: csv/xlsx with columns]

%% 3) Query flow (query command) – Advanced LLM + ANN + Fuzzy
flowchart TB
  A[run_query] --> B[load_bundle - index_dir]
  A --> C[configure_cpu_threading + threadpool_limits - nullcontext when unused]
  A --> D{use_llm_clean or use_llm_rerank?}

  D -- no --> LEG[Legacy Path]
  D -- yes --> E[Init LLMClient - vLLM or llama-cpp]

  %% Expansion
  E --> F[llm_expand_query - EXPAND_* prompts]
  F --> G[Candidates - alternate_keywords,\ndescription, possible_stys]
  G --> H[build_expanded_summary]

  %% Embedding signals
  H --> I[embed_texts - query -> q_vec]
  G --> J[embed_texts - descriptions per cand]
  G --> K[embed_texts - alt_keywords batched]
  G --> L[precompute_sty_scores_map - pred_stys, STY vocab/emb]

  %% ANN searches on both systems
  subgraph S[Semantic Searches]
    direction LR
    I --> S1[FAISS search top_direct per system]
    J --> S2[FAISS search top_desc per system]
    K --> S3[FAISS batch search top_kw per system]
  end

  S1 --> M[Aggregate row_scores by - system,row_id]
  S2 --> M
  S3 --> M
  L --> M

  M --> N[Compute composite score per row w_desc, w_kw, w_direct, w_sty]
  N --> O[Take top fuzzy_pool by composite]

  %% Fuzzy
  O --> P{HAVE_RAPIDFUZZ?}
  P -- yes --> Q[rapidfuzz two-stage max - query + KWs, STR]
  P -- no --> R[skip]
  Q --> S0[Re-sort by fuzzy desc]
  R --> S0
  S0 --> T[Take top_pool_after_fuzzy]

  %% Aggregate per code
  T --> U[Group by - System, CODE]
  U --> V[Recompute composite per member row using memmapped vectors - q_vec, best desc, all KWs, STY map]
  V --> W[Compute per-code: avg_all, avg_500, pct_in_top500, final = avg_all * avg_500 * sqrt log10 pct]
  W --> X[Pick representative term per code - PT/FN/SCD/SBD preference]
  X --> Y{use_llm_rerank?}
  Y -- yes --> Z[llm_rerank_codes - RERANK_* prompts -> ordered codes]
  Y -- no --> Z0[Keep top final by score]

  Z --> AA[Emit top-k rows]
  Z0 --> AA

  %% Legacy branch
  LEG --> L1[embed_texts - query]
  L1 --> L2[FAISS search SNOMED + RxNorm]
  L2 --> L3[build_hits_simple - optional RapidFuzz]
  L3 --> L4[choose_system_hits - preferred vs alt]
  L4 --> AA
  AA --> OUT[stdout: Top-k printed]


Future works info:
for future works we can add things mentioned and things like adding more random connections in hnsw to make the world more smaller (small world thing states even ~1% random connections can make the world whole lot smaller, and adding some exploration bias to farther nodes from current ones, would lead more overall exploration possibilities) (take redernece from watts and strogatz, collective dynamics of small world networks)
also add things already mentioned, like frequency, caching, further optimisations possibilities and all.
one more thing i was thinking about was that, we have added llm for query expansion and re-ranking but we can also do things like llm finetuning on this data based on RL (GRPO), generate synthetic qa pairs and ask llm to reason about the correct thing, give reward to correct things and penalise wrong ones, also in case performance is too poor for cold start rl, while generating synthetic qa pairs we can also geenrate a sample reasoning+response pair for the sample that why for query a answer should be b, and then sft the llm on that sample, and then do RL + rag applications for better equipped answers + agentic exploration.
this was all llms from query side, we can also add llms from build side, as in while building we can extract rich descriptions and alt keywords for each unique code, and while querying, instead of matching against very unclean and mixed info/std descriptions, we can match against very rich descriptions and larger list of possible keywords. this will be expensive but one time, there are ~1.4 million entries and ~600K unique codes, supposing ~10 codes per llm call, then we would need ~60K llm api calls but that would be a onetime load, which has the potential on significantly improving accuracy.
you may also add any other idea if you have.
we can add other similarity matching systems (lexical) like bm25. we can also rely more on clustering related techniques/tree-like division of the dataset, in terms of sty, keywords, etc.

additional context (provided by chatgpt, with whom i had created everything till now in collaboration with, which thought that this info will be crucial for you to create the website. chatgpt had all context about the ps, the explanations, building the solution, etc., on everything it had info about, so it is providing you with context which it thinks maybe relevant):
# Supplemental spec for the website generator

## 0) TL;DR (what to build)

Create a public, static site that explains and demos **Clinical Concept Harmonizer** — a hybrid engine mapping messy inputs to **RxNorm** (meds) and **SNOMED CT** (diagnoses/procedures/labs) using:

* Semantic search (embeddings + FAISS)
* Fuzzy matching (RapidFuzz) with a two-stage pass
* Optional LLM assist (query expansion + final rerank)
* Per-code aggregation with stability boosts
  No PHI is persisted; all inputs are processed in-memory.

## 1) Mini-glossary (use these exact wordings for “?” tooltips)

* **Anchor**: a string we compare candidates against. We use multiple anchors: `{normalized user query} ∪ top alternate keywords`. Final fuzzy score = **max** across anchors.
* **ratio (prefilter)**: RapidFuzz’s fast edit similarity (0–100). Used to shortlist.
* **token_set_ratio (refine)**: set-based similarity (order/duplicates robust). Used on the shortlist only.
* **STY (Semantic Type)**: clinical category (e.g., Disease/Syndrome, Pharmacologic Substance). We compute `max cos(pred_STYs, sty_vocab)` to score compatibility.
* **cos→[0,1]**: normalize cosine: `cos_to_01(x)=clip((x+1)/2, 0, 1)` so all signals share the same 0–1 scale.
* **snomed_vectors.f32 / rxnorm_vectors.f32**: memory-mapped float32 matrices with **normalized** embeddings; row-aligned to `row_id`. Loaded for fast vector ops at runtime.
* **HNSW efSearch**: “how exhaustively to search” in the HNSW graph; higher = better recall, slower.
* **Thread knobs**: `--blas_threads` caps NumPy/BLAS pools; `--faiss_threads` sets FAISS OpenMP threads; `--fuzzy_workers` sets RapidFuzz parallelism.
* **CUDA alloc (`PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True`)**: lets PyTorch grow memory segments to reduce fragmentation & OOMs.

## 2) Required sections (single page with anchored nav)

* **Hero**: one-line value prop + CTAs “See Pipeline”, “Try Demo”, “Download Repo”.
* **Problem → Solution**: use hackathon examples (Hb/HGB, Paracetamol/Acetaminophen, etc.). Short by default; “?” popovers for details.
* **Architecture (Mermaid)**: render repo flow files; if none exist, use this default:

```mermaid
flowchart LR
  A[Raw input] --> B[LLM expansion (alt keywords, brief desc, STYs)]
  B --> C[Semantic search over SNOMED & RxNorm (FAISS)]
  C --> D[Weighted mix: desc + kw + direct + STY]
  D --> E[Two-stage fuzzy: ratio prefilter -> token_set_ratio refine]
  E --> F[Per-code aggregation & boost]
  F --> G{Optional LLM rerank}
  G --> H[Top-k codes (system | code | STR | STY | TTY)]
```

* **How scoring works**: sliders for 4 signals (defaults: 0.30/0.40/0.20/0.10) + formula chip.
* **Demo (client-side only)**: textbox + entity-type dropdown; run a **mock** scoring over a tiny hardcoded JSON; show ranked cards (System, Code, STR, STY, TTY) + “Why this?” popover explaining anchors + fuzzy stages.
* **Performance & knobs**: cards for `efSearch`, `--blas_threads`, `--faiss_threads`, `--fuzzy_workers` (one-line description + “learn more” tooltip).
* **Data safety**: “No PHI persisted; inputs processed in-memory.”
* **Future work**: incremental indexing, domain adapters, confidence intervals, site whitelists/blacklists, code priors, usage frequency, caching expansions.
* **Credits & licensing**: FAISS, SentenceTransformers, RapidFuzz, vLLM/llama-cpp, SNOMED/RxNorm notes.

## 3) UX rules (must-haves)

* Monochrome (black/white/gray) like OpenAI/Uber; large type, spacious layout, subtle motion.
* Progressive disclosure: every dense concept has a `?` (hover tooltip; click popover on mobile).
* Auto-render all `.mmd`/`.mermaid` under `/flows` or `/diagrams`; fall back to default diagram above.
* Syntax-highlight short code; compact tables for sample I/O.
* Accessibility: visible focus, ≥4.5:1 contrast, aria-labels on icons.

## 4) Tech constraints (GitHub Pages)

* Static only; all logic runs client-side.
* Allowed CDNs: Mermaid, Prism.js (or highlight.js), Tippy.js (or tiny custom tooltip), optional Tailwind (CDN).
* Ship structure:

```
/
├─ index.html
├─ /assets/styles.css
├─ /assets/app.js
├─ /data/samples.json
├─ /diagrams/*.mmd
└─ /img/*
```

* Mermaid init (once, before rendering):

```html
<script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
<script>mermaid.initialize({ startOnLoad: true, securityLevel: 'loose', theme: 'neutral' });</script>
```

* Deploy: output to `docs/` (or repo root) for GitHub Pages; add a short “How to deploy” note in the footer.

## 5) Content to extract from the repo

* `README.md` & `Arch.md`: copy blocks for Problem, Approach, Method, Knobs, Troubleshooting.
* Flow files (Mermaid): auto-embed as diagrams.
* Sample I/O from the problem statement: drive the demo table + `/data/samples.json`.
* Short explainers must cover: anchors & two-stage fuzzy (ratio → token_set_ratio); STY compatibility (`max cos(pred_STYs, sty_vocab)`); per-code aggregation `final = mean(all) × mean(top-pool) × sqrt(log10(% in pool))`; cosine normalization to [0,1]; meanings of `efSearch`, thread knobs, CUDA expandable segments; purpose of `*.f32` mem-mapped vectors.

## 6) Demo data (use if none provided)

Create `/data/samples.json`:

```json
{
  "examples": [
    { "text": "Paracetamol 500 mg", "entity_type": "Medication",
      "candidates": [
        { "system": "RXNORM", "code": "198440", "str": "Acetaminophen 500 MG Oral Tablet", "sty": "Pharmacologic Substance", "tty": "SCD" },
        { "system": "RXNORM", "code": "161", "str": "Acetaminophen", "sty": "Pharmacologic Substance", "tty": "IN" }
      ],
      "anchors": ["paracetamol 500 mg", "acetaminophen 500 mg", "tylenol 500"]
    },
    { "text": "Chest xr", "entity_type": "Procedure",
      "candidates": [
        { "system": "SNOMEDCT_US", "code": "399208008", "str": "Plain X-ray of chest", "sty": "Diagnostic Procedure", "tty": "PT" }
      ],
      "anchors": ["chest xr", "chest x-ray", "cxr", "chest radiograph"]
    }
  ]
}
```

The client-side demo **simulates** scoring: show anchors; show a faux two-stage fuzzy run (stage-1 keeps N, stage-2 ranks); display top-k with badges; “Why this?” popover: “max token_set_ratio across anchors after ratio prefilter”.

## 7) Copy deck (ready-to-use snippets)

* **Value prop**: “Normalize messy clinical inputs to standardized **RxNorm** and **SNOMED CT**—fast, robust, and explainable.”
* **Fuzzy explainer**: “We first run a fast **ratio** prefilter per anchor to shrink the pool, then refine with **token_set_ratio**; final score = **max** across anchors.”
* **STY explainer**: “We compare predicted semantic types against a pre-embedded STY vocabulary: **max cosine**; higher means the candidate fits the intended clinical category.”
* **Safety**: “No PHI is stored; inputs are processed in memory only.”

## 8) Visual component specs

* **Weight sliders**: four sliders (desc/kw/direct/sty), defaults 0.30/0.40/0.20/0.10; display current sum.
* **Result card**: System (pill), Code, STR, STY (chip), TTY (chip) + “?” popover.
* **Knob cards**:

  * efSearch — “HNSW search breadth. ↑ = better recall, slower.”
  * BLAS threads — “Max CPU threads for NumPy/BLAS math.”
  * FAISS threads — “Parallelism for ANN search.”
  * Fuzzy workers — “RapidFuzz parallel batch size.”
* **Theme**: white bg, black text, gray accents; 12/16/24 spacing; rounded-2xl cards; micro-hover shadow.

## 9) Implementation checklist

* Page is readable with JS disabled (headings + static text; diagrams show “Load to view” placeholder).
* On load, Mermaid renders all `.mmd` under `/diagrams`.
* Demo works **offline** using `/data/samples.json`.
* Every “?” explains: anchors; ratio vs token_set_ratio; STY map; cosine normalization; efSearch; threads; CUDA expandable segments.
* Lighthouse ≥ 90 for Performance & Accessibility.
* Output suitable for GitHub Pages (`docs/` or root).
* No external network calls beyond whitelisted CDNs.
* Footer includes “How to run / deploy”.

## 10) Pitfalls to avoid

* Don’t invent medical codes; use provided examples.
* Don’t present the demo as live mapping; label it **simulation** with sample data.
* Keep the palette monochrome.
* Mirror repo diagrams/formulas; don’t rely only on LLM prose.

## 11) Optional nice-to-haves

* Toggle to show **LLM-off** vs **LLM-on** narratives.
* Mini CLI section with copy-buttons for `build`, `batch`, `query`.
* Tiny “Metrics” strip: catalog sizes, index type, default `efSearch`.


now go and create a nicely designed beautiful website.